\documentclass[coursework]{SCWorks}
% Тип обучения (одно из значений):
%    bachelor   - бакалавриат (по умолчанию)
%    spec       - специальность
%    master     - магистратура
% Форма обучения (одно из значений):
%    och        - очное (по умолчанию)
%    zaoch      - заочное
% Тип работы (одно из значений):
%    coursework - курсовая работа (по умолчанию)
%    referat    - реферат
%  * otchet     - универсальный отчет
%  * nir%ournal - журнал НИР
%  * digital    - итоговая работа для цифровой кафдры
%    diploma    - дипломная работа
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% Включение шрифта
%    times      - включение шрифта Times New Roman (если установлен)
%                 по умолчанию выключен
\usepackage{preamble}
\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Построение и сравнение тезаурусов на основе анализа текстовых данных новостных источников}

% Курс
\course{3}

% Группа
\group{351}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
% \department{факультета КНиИТ}

% Специальность/направление код - наименование
% \napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
% \napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
% \napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
% \napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
% \studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Янченко Вадима Александровича}

% Заведующий кафедрой 
\chtitle{доцент, к.\,ф.-м.\,н.}
\chname{С.\,В.\,Миронов}

% Руководитель ДПП ПП для цифровой кафедры (перекрывает заведующего кафедры)
% \chpretitle{
%     заведующий кафедрой математических основ информатики и олимпиадного\\
%     программирования на базе МАОУ <<Ф"=Т лицей №1>>
% }
% \chtitle{г. Саратов, к.\,ф.-м.\,н., доцент}
% \chname{Кондратова\, Ю.\,Н.}

% Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Руководитель практики от организации (руководитель для цифровой кафедры)
\patitle{доцент, к.\,ф.-м.\,н.}
\paname{С.\,В.\,Миронов}

% Руководитель НИР
\nirtitle{доцент, к.\,п.\,н.} % степень, звание
\nirname{В.\,А.\,Векслер}

% Семестр (только для практики, для остальных типов работ не используется)
\term{2}

% Наименование практики (только для практики, для остальных типов работ не
% используется)
\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики, для
% остальных типов работ не используется)
\duration{2}

% Даты начала и окончания практики (только для практики, для остальных типов
% работ не используется)
\practStart{01.07.2022}
\practFinish{13.01.2023}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам (по умолчанию -
% нумерация сквозная) (допускается оба вида нумерации)
\secNumbering

\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item ... "--- ...
%     \item ... "--- ...
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
% \definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и
% "Определения"
% \defabbr

\intro
В задачах обработки естественного языка (NLP) и информационного поиска (IR) важную роль играет использование различных видов знаний: лексических связей между словами, значений слов, специализированных понятий в предметных областях и знаний. Одним из традиционных способов представления такого рода знаний в системах NLP являются тезаурусы\cite{loukachevitch2021ruthes}.

В контексте компьютерной обработки текста тезаурус представляет собой формализованный ресурс, в котором описаны семантические связи между словами или терминами — например, синонимы, гипонимы и другие виды лексических отношений. Благодаря такой структуре, тезаурусы могут использоваться в автоматизированных системах для анализа, поиска и интерпретации текстовой информации.

Использование готовых тезаурусов, обученных на огромном количестве произведений, не всегда даёт лучший результат из"=за особенностей предметной области. Такие ресурсы, как правило, отражают общую лексику языка, но не учитывают контекстуальные особенности и терминологию конкретных тематик.

В связи с этим всё более актуальным становится автоматическое построение тезаурусов на основе анализа конкретных текстов. Такой подход позволяет выявить семантические связи, характерные именно для выбранного корпуса, что способствует более точному и релевантному представлению знаний.

Цель данной курсовой работы "--- построить и сравнить тезаурусы, созданные с помощью методов PMI, Word2Vec, GloVe и BERT, на основе корпуса новостных текстов. Это позволит оценить, насколько различаются семантические представления в зависимости от выбранной модели и насколько точно каждый подход отражает смысловые связи в реальных текстах.

Для достижения поставленной цели необходимо решить следующие задачи:
\begin{itemize}
  \item изучить методы PMI, Word2Vec, GloVe и BERT в построении тезаурусов;
  \item построить тезаурусы с использованием этих методов;
  \item провести сравнительный анализ полученных тезаурусов
\end{itemize}

\section{Понятие тезауруса}
В информационном поиске часто прибегают к механизму расширения поискового запроса. Заключается он в добавлении связанных терминов или понятий к исходному запросу, по сути, переписывая его. Это помогает найти документы, которые могут не совпадать с исходными ключевыми словами, но при этом удовлетворять информационные потребности пользователя.

Для поиска синонимов или близких по смыслу слов прибегают к использованию тезауруса. Тезаурус представляет собой структурированный лексический ресурс, в котором фиксируются семантические отношения между словами и терминами. В отличие от обычных словарей, тезаурусы явно отображают связи между ними — такие как синонимия, антонимия, родо-видовые отношения, ассоциативные связи и другие.

Современные тезаурусы могут быть как ручными (созданными экспертами в конкретной предметной области), так и автоматически построенными на основе анализа текстов с применением статистических и нейросетевых моделей. Автоматические тезаурусы особенно актуальны в условиях быстро меняющейся лексики, характерной, например, для новостных источников.

Одним из наиболее известных лексических ресурсов в сфере компьютерной 
лингвистики  и автоматической обработки текстов является компьютерный тезаурус WordNet. WordNet версии 3.0  включает приблизительно 155 тысяч различных лексем и словосочетаний, организованных в 117 тысяч понятий, или совокупностей синонимов (synset), общее число пар лексема – значение составляет более 200 тысяч.

Основным отношением в WordNet является отношение синонимии. Наборы 
синонимов "--- синсеты "--- являются основными структурными элементами WordNet.  
Понятие синонимии, используемое разработчиками WordNet, базируется на критерии, что два выражения являются синонимичными, если замена одного из них на другое в предложении не меняет значения истинности этого высказывания.

% https://istina.msu.ru/media/publications/book/171/c90/1283494/louk_book.pdf

При этом не требуется заменяемости синонимов во всех контекстах – по такому 
критерию в естественном языке было бы слишком мало синонимов. Используется 
значительно более слабое утверждение, что синонимы WordNet должны быть 
взаимозаменимы хотя бы в некотором множестве контекстов. Например, замена plank (доска, планка) для слова board (доска) редко меняет значение истинности в контексте плотницкого дела, но существуют контексты, где такая замена не может считаться приемлемой.

\section{Построение тезауруса}

Современные методы автоматического построения тезаурусов базируются на анализе больших текстовых корпусов и извлечении статистических или контекстуальных связей между словами. Основная идея таких подходов заключается в том, что слова, встречающиеся в схожих контекстах, как правило, имеют близкие значения. Возникает проблема отражения смысла слова.

Начнём с рассмотрения слова (возьмём, например, слово \textit{рама}). Слово \textit{рама} является леммой, то есть находится в начальной форме слова. Форма инфинитива используется в качестве леммы для глагола. Конкретные слова \textit{рамы} или \textit{мыли} являются словоформами.

Каждая лемма может иметь несколько значений. Тот факт, что леммы обладают полисемией (многозначностью) может затруднить интерпретацию. 

Кроме того, возможны случаи, когда одно слово имеет смысл, значение которого индентично смыслу другого слова или почти идентично. 
Такие слова называют синонимами. Примеры синонимов: \textit{кавалерия} "--- \textit{конница}, \textit{смелый} "---\textit{храбрый}.

Более формальное определение синонимии следующее: два слова являются синонимами, если они заменяют друг друга в любом предложении без изменения условий истинности предложения, то есть ситуаций, в которых предложение будет истинным.

На практике же два слова вероятно не обладают абсолютно идентичным смыслом. Слова могут различаться оттенками в значениях (\textit{мокрый} "--- \textit{влажный}) или употребляться в разных сферах (\textit{жена} (общеупотр.)"--- \textit{супруга} (офиц.)).

Хотя у слов не так много синонимов, у большинства слов есть много похожих слов. Кошка не является синонимом собаки, но кошки и собаки - это, безусловно, похожие слова. При переходе от синонимии к сходству будет полезно перейти от разговора об отношениях между смыслами слов (как при синонимии) к отношениям между словами (как при сходстве). Работа со словами позволяет избежать необходимости придерживаться определенного представления смыслов слов, что, как выясняется, упрощает нашу задачу. 

Понятие сходства слов очень полезно в больших семантических задачах. Знание того, насколько похожи два слова, может помочь в вычислении того, насколько схожи значения двух фраз или предложений.
% \subsection{Статистический метод. PMI}
% Одним из простейших и наиболее понятных методов извлечения семантических связей является использование метода поточечной взаимной информации (PMI). 

% Если говорить в общем, PMI - это метрика ассоциации, которая сравнивает относительную частоту двух исходов, происходящих вместе, с вероятностью того, что любой из них произойдет независимо.
\subsection{Word2Vec}
Ключевая идея состоит в представлении слова как вектора в многомерном пространстве. Самый простой способ перехода от слова к вектору состоит в следующем. Пусть $V$ "--- множество всех слов. Слово $w_i \in V$ представим как вектор размера $n = |V|$, в котором все координаты равны $0$ кроме $i$"=ой координаты, которая равна $1$.

Рассмотрим другой способ представление слова как вектора "--- эмбеддинг. Эмбеддинги имеет количество размерностей $d$ от 50 до 1000, что существенно меньше количества слов $|V|$ во всех документах. Кроме того, эти векторы плотные: вместо того, чтобы большинство координат были равны 0, значения будут вещественными числами, которые могут быть и отрицательными.

Эмбеддинги Word2Vec являются статическими, это означает, что каждому слову соответствует ровно один эмбеддинг. Такие модели как BERT позволяют создавать контекстуальные эмбеддинги, которые сопоставляют разный вектор к слову в зависимости от контекста.

Word2Vec реализуется в двух основных архитектурах: \textbf{CBOW} и \textbf{Skip"=Gram}.

\subsubsection{Архитектура модели Skip-Gram}

Модель Skip-Gram является одной из двух основных архитектур Word2Vec и направлена на предсказание контекстных слов по центральному слову. То есть, имея слово $w_t$, модель обучается предсказывать слова, находящиеся в некотором окне вокруг него: $w_{t-c}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+c}$, где $c$ — размер окна.


Модель Skip-Gram представляет собой простую нейросеть с одним скрытым слоем. Она состоит из следующих компонентов:

\begin{enumerate}
  \item \textbf{Входной слой.}  
  Каждое слово из словаря $V$ кодируется как one-hot вектор $\mathbf{x} \in \mathbb{R}^{|V|}$, в котором все элементы равны нулю, кроме одного, соответствующего индексу слова $w$.

  \item \textbf{Скрытый слой.}  
  Представлен матрицей весов $W \in \mathbb{R}^{|V| \times d}$, где $d$ — размерность эмбеддингового пространства. Преобразование входа:
  \[
  \mathbf{h} = \mathbf{x}^\top W
  \]
  Вектор $\mathbf{h} \in \mathbb{R}^d$ и является искомым эмбеддингом слова $w$.

  \item \textbf{Выходной слой.}  
  Представлен второй матрицей весов $W' \in \mathbb{R}^{d \times |V|}$. Результат выходного слоя:
  \[
  \mathbf{u} = W'^\top \mathbf{h}
  \]
  где $\mathbf{u} \in \mathbb{R}^{|V|}$ — логиты для каждого слова в словаре.

  \item \textbf{Softmax.}  
  На выходе применяется softmax-функция, преобразующая логиты в вероятности:
  \[
  P(w_O \mid w_I) = \frac{\exp(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I})}{\sum_{w \in V} \exp(\mathbf{v}'_w \cdot \mathbf{v}_{w_I})}
  \]
  Здесь:
  \begin{itemize}
    \item $w_I$ — входное (центральное) слово;
    \item $w_O$ — слово из контекста;
    \item $\mathbf{v}_{w_I}$ — вектор из матрицы $W$ (входной эмбеддинг);
    \item $\mathbf{v}'_{w_O}$ — вектор из матрицы $W'$ (выходной эмбеддинг).
  \end{itemize}
\end{enumerate}

Рассмотрим функцию потерь.
Для одного примера $(w_I, w_O)$ используется кросс-энтропия:
\[
\mathcal{L} = -\log P(w_O \mid w_I)
\]
Общая функция потерь по корпусу:
\[
\mathcal{L}_{\text{total}} = -\sum_{t=1}^{T} \sum_{\substack{-c \leq j \leq c \\ j \ne 0}} \log P(w_{t+j} \mid w_t)
\]
где $T$ — длина корпуса.

Эмбеддингом является как матрица ($W$), так и матрица($W'$) — часто используется только $W$. 

Что стоит отметить: хотя в модель не заложено явно никакой семантики, а только статистические свойства корпусов текстов, оказывается, что натренированная модель word2vec может улавливать некоторые семантические свойства слов. Классический пример: cлово <<мужчина>> относится к слову <<женщина>> так же, как слово <<дядя>> к слову <<тётя>>, что для нас совершенно естественно и понятно, но в других моделям добиться такого же соотношения векторов можно только с помощью специальных ухищрений. Здесь же — это происходит естественно из самого корпуса текстов.

\subsubsection{Negative sampling}
Полноценный расчёт softmax-функции в модели Skip-Gram требует вычисления скалярного произведения с каждым словом в словаре, что приводит к высокой вычислительной сложности: $\mathcal{O}(|V|)$ на каждый пример.

Для решения этой проблемы в модели Word2Vec используется приближённый метод оптимизации "--- negative sampling.

Вместо того чтобы обучать модель различать целевое слово $w_O$ от всех остальных слов в словаре $V$, negative sampling предлагает:
\begin{enumerate}
  \item обучать модель различать настоящие пары (центральное слово и слово из его контекста),
  \item и несколько случайных пар (центральное слово и случайные, нерелевантные слова), которые считаются негативными примерами.
\end{enumerate}

Пусть:
\begin{itemize}
  \item $w_I$ — центральное слово (input word),
  \item $w_O$ — контекстное слово (output word, положительный пример),
  \item $w_1^-, w_2^-, \dots, w_K^-$ — отрицательные примеры, выбранные случайным образом из словаря по заданному распределению.
\end{itemize}

Тогда оптимизируется следующая функция потерь:

\[
\mathcal{L} = \log \sigma(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I}) + \sum_{k=1}^{K} \mathbb{E}_{w_k^- \sim P_n(w)} \left[ \log \sigma(-\mathbf{v}'_{w_k^-} \cdot \mathbf{v}_{w_I}) \right]
\]

где:
\begin{itemize}
  \item $\sigma(x) = \frac{1}{1 + e^{-x}}$ — сигмоида;
  \item $\mathbf{v}_{w_I}$ — входной эмбеддинг слова $w_I$;
  \item $\mathbf{v}'_{w_O}$ — выходной эмбеддинг контекстного слова $w_O$;
  \item $P_n(w)$ — распределение, по которому выбираются отрицательные примеры;
  \item $K$ — число отрицательных примеров.
\end{itemize}

Для выбора отрицательных слов используется неравномерное распределение. По умолчанию (в оригинальной статье Word2Vec) это распределение частот слов в степени $3/4$:

\[
P_n(w) = \frac{f(w)^{3/4}}{\sum_{w' \in V} f(w')^{3/4}}
\]

где $f(w)$ — частота слова $w$ в корпусе.

% Вкратце Skip"=Gram заключается в следующем:
% \begin{enumerate}
%   \item Рассматривается центральное слово и окружающий его контекст. 
%   \item Ставится задача классификации
%   \item Количество классов "--- размер словаря $|V| = n$.
%   \item На вход нейросеть принимает слово, выдает $n$ значений "--- распределение на слова в словаре.
%   \item Функция потерь "--- кросc"=энтропия между распределением, выданным сетью, и верным распределением (one"=hot вектор)
% \end{enumerate}


  
\subsection{GloVe}
% GloVe (Global Vectors) — это другая модель построения эмбеддингов.

% Пусть матрица $X$ "--- матрица ко"=частотности (co-occurrence), где $X_{ij}$ показывает, сколько раз слово $j$ встречается в контексте слова $i$. Пусть $X_i = \sum_k X_{ik}$ "--- количество всех слов, встречающихся в контексте слова $i$.
% Пусть $P_{ij} = P(j | i) = X_{ij} / X_i$ будет вероятностью того, что слово $j$ встретилось в контексте слова $i$.

Модель GloVe (Global Vectors) предназначена для построения векторных представлений слов на основе глобальной статистики совместных появлений слов в тексте. В отличие от моделей Word2Vec, которые обучаются на локальных контекстных окнах, GloVe использует информацию о числе совместных появлений слов во всём корпусе, агрегируя её в специальную матрицу.

На первом этапе строится матрица совместных появлений $X \in \mathbb{R}^{|V| \times |V|}$, где $X_{ij}$ — количество раз, когда слово $j$ встречается в контексте слова $i$. Контекст обычно ограничивается окном фиксированного размера (например, 5 слов влево и вправо), при этом можно учитывать взвешивание по расстоянию до центрального слова.

GloVe обучает два векторных представления: $w_i \in \mathbb{R}^d$ — для центрального слова $i$ и $\tilde{w}_j \in \mathbb{R}^d$ — для контекстного слова $j$, а также два скалярных смещения $b_i$ и $\tilde{b}_j$. Обучение направлено на аппроксимацию следующего равенства:

\[
w_i^\top \tilde{w}_j + b_i + \tilde{b}_j \approx \log X_{ij}
\]

Целевая функция, которую минимизирует модель, записывается в следующем виде:

\[
J = \sum_{i=1}^{|V|} \sum_{j=1}^{|V|} f(X_{ij}) \left(w_i^\top \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2
\]

где $f(X_{ij})$ — весовая функция, ограничивающая влияние часто встречающихся пар слов, например:

\[
f(x) = 
\begin{cases}
\left(\frac{x}{x_{\text{max}}}\right)^\alpha & \text{если } x < x_{\text{max}} \\
1 & \text{иначе}
\end{cases}
\]

На практике используются значения $\alpha = 0{,}75$ и $x_{\text{max}} = 100$.

% Обучение проводится методом стохастического градиентного спуска или его модификаций (например, AdaGrad). На каждом шаге параметры $w_i$, $\tilde{w}_j$, $b_i$, $\tilde{b}_j$ обновляются таким образом, чтобы минимизировать ошибку между скалярным произведением векторов и логарифмом числа совместных появлений.

После завершения обучения итоговое представление слова $i$ получается как сумма обученных векторов:

\[
v_i = w_i + \tilde{w}_i
\]

Таким образом, каждая лексема кодируется плотным вектором фиксированной размерности, отражающим её глобальные статистические связи с другими словами корпуса.


% Word2Vec фокусируется на локальном контексте, обучаясь на парах центр-контекст, и опирается на информацию из небольших окон. В отличие от неё, GloVe использует глобальную статистику словоупотребления по всему корпусу, строя векторные представления на основе матрицы ко-частотности (co-occurrence matrix) — то есть того, насколько часто слова встречаются рядом друг с другом.

% Это позволяет GloVe более явно учитывать семантические связи между словами, которые могут не быть очевидными в локальных контекстах.

% Мб добавить + PMI
% \subsection{Контекстуальные модели. BERT}

% \section{Методы сравнения тезаурусов}

\section{Практическая часть}
В рамках практической части осуществляется построение тезауруса на основе новостей сайтов СГУ и СГТУ. В процессе построения используются описанные выше модели Word2Vec и GloVe.

Весь программный код написан на языке программирования python версии $3.12.6$. В программном коде используются следующие библиотеки:
\begin{itemize}
  \item pandas для удобного управления данными;
  \item gensim для работы с моделями
  % \item 
\end{itemize}
\subsection{Предварительная обработка данных}
Рассмотрим данные, которые представляют из себя новости, взятые из официального сайта СГУ. Каждый документ характеризуется следующими параметрами:
\begin{enumerate}
  \item дата написания новости;
  \item заголовок новости;
  \item необработанный текст новости;
  \item обработанный текст новости;
  \item количество слов;
  \item количество токенов.
\end{enumerate}

Обработка текста заключается в удалении стоп"=слов и лемматизации слов.

Стоп-слова — это слова, которые встречаются в языке чрезвычайно часто, но, как правило, не несут самостоятельной смысловой нагрузки в контексте информационного поиска или анализа текста. К таким словам относятся, например, союзы, предлоги, частицы, местоимения: «и», «в», «на», «что», «как», «это», и т.п. Их исключение позволяет снизить «шум» в текстах, повысить эффективность алгоритмов и сосредоточиться на лексемах, действительно значимых для понимания содержания документа.
  
Лемматизация, в свою очередь, представляет собой процесс приведения слова к его начальной, словарной форме — лемме. Это позволяет уменьшить количество уникальных словоформ в тексте, объединив различные грамматические формы одного и того же слова (например, «пошёл», «идёт», «шли» — к лемме «идти»). Благодаря лемматизации повышается точность анализа, поскольку слова с одинаковым значением, но разными формами, будут обрабатываться как единое понятие.

В новостях часто встречаются имена людей, городов, стран, наименования различных компаний, которые не привносят смысл в предложение, поэтому от этого также стоит избавиться.

Программный код, выполняющий обработку текста, можно рассмотреть в приложении \ref{apx:tokenization}.

Ознакомиться со статистикой полученных данных можно в табл. \ref{table:preprocess}.
\begin{table}[!h]
    \centering
    % \footnotesize
    \begin{tabular}{|l|l|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Статистика}}  & \multicolumn{1}{c|}{\textbf{Значение}} \\ \hline
    Количество новостей                        & 31077                                  \\ \hline
    Общее количество предложений               & 423175                                 \\ \hline
    Среднее количество предложений на новость  & 13.62                                  \\ \hline
    Общее количество токенов                   & 4293732                                \\ \hline
    Среднее количество токенов на новость      & 138.16                                 \\ \hline
    Максимальное количество токенов на новость & 166                                    \\ \hline
    Минимальное количество токенов на новость  & 1                                      \\ \hline
    Количество уникальных токенов              & 45204                                  \\ \hline
    \end{tabular} 
    \caption{Статистика по полученным данным}
    \label{table:preprocess}
\end{table}

\subsection{Построение тезаурусов}
\subsubsection{Word2Vec}
Обучим модель Word2Vec на полученных данных. Для этого напишем следующую программу:
\begin{minted}{python3}
import dask.dataframe as dd
import dask.bag as db
from gensim.models import Word2Vec
docs = dd.read_parquet("../output.pq/")
texts = docs['News_Tokens'].compute()
bag = db.from_sequence(texts)
sentences = bag.flatten().map(lambda x: x.split()).compute()
model = Word2Vec(sentences=sentences, vector_size=250, window=7,min_count=5, sg=0)
model.save('word2vec_sent_5_sgu.model')
\end{minted}

Получив модель, построим тезаурус:
\begin{minted}{python3}
import pandas as pd
import dask.dataframe as dd
import dask.bag as db
from gensim.models import Word2Vec
docs = dd.read_parquet("../output.pq/")
model = Word2Vec.load("word2vec_sent_5_sgu.model")
texts = docs['News_Tokens'].compute()
bag = db.from_sequence(texts)
sentences = bag.flatten()
words = sentences.map(lambda x: x.split()).flatten().distinct().compute()
results = []
threshold = 0.7
for word in words:
    try:
        # Получаем 10 ближайших слов
        similar_words = model.wv.most_similar(word, topn=10)
        filtered_words = [(w, sim) for w, sim in similar_words if sim >= threshold]
        results.append([word, filtered_words])
    except KeyError:
        # Если слово не в модели, пропускаем его
        print(f"Слово '{word}' не найдено в модели.")

df_results = pd.DataFrame(results, columns=["Word", "Most_Similar_Word"])

df_results.to_csv("similar_words_sent_5.csv", index=False)
\end{minted}
Первые 10 строчек полученного файла:
\begin{table}[!h]
    \centering
    \footnotesize
    \begin{tabular}{|l|l|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Word}} & \multicolumn{1}{c|}{\textbf{Most\_Similar\_Word}}                                                                                                                                                                                                                                                                                                                                             \\ \hline
    май                                 & \begin{tabular}[c]{@{}l@{}}"{[}('февраль', 0.8803005218505859), ('октябрь', 0.8746454119682312), ('апрель', 0.8725210428237915), \\   ('ноябрь', 0.857973039150238), ('декабрь', 0.8578871488571167), ('июнь', 0.8487043380737305), \\   ('март', 0.8360908627510071), ('январь', 0.8248278498649597), ('сентябрь', 0.8231253623962402), \\   ('август', 0.7303727865219116){]}"\end{tabular} \\ \hline
    состояться                          & \begin{tabular}[c]{@{}l@{}}"{[}('пройти', 0.8558952808380127), ('завершиться', 0.7617708444595337), \\    ('открыться', 0.7213159799575806), ('проходить', 0.7007029056549072){]}"\end{tabular}                                                                                                                                                                                               \\ \hline
    акция                               & {[}{]}                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
    бессмертный                         & "{[}('полк', 0.949421226978302), ('георгиевский', 0.7148336172103882){]}"                                                                                                                                                                                                                                                                                                                     \\ \hline
    полк                                & "{[}('бессмертный', 0.949421226978302){]}"                                                                                                                                                                                                                                                                                                                                                    \\ \hline
    который                             & {[}{]}                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
    принять                             & "{[}('принимать', 0.8259356021881104){]}"                                                                                                                                                                                                                                                                                                                                                     \\ \hline
    участие                             & {[}{]}                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
    студент                             & {[}{]}                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
    \end{tabular}
    \caption{Первые 10 строчек тезауруса, построенного моделью Word2Vec}
\end{table}
\subsubsection{GloVe}
Для создания модели GloVe склонируем их официальный репозиторий\footnote{https://github.com/stanfordnlp/GloVe}. Далее выполним следующие команды:
\begin{minted}{bash}
build/vocab_count -min-count 5 -verbose 2 < corpus.txt > vocab.txt
build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 7 < corpus.txt >cooccurrence.bin
build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin
build/glove -save-file vectors -threads 12 -input-file cooccurrence.shuf.bin -x-max 10 -iter 15 -vector-size 250 -binary 2 -vocab-file vocab.txt -verbose 2
\end{minted}

По полученной модели построим тезаурус:
\begin{minted}{python3}
import pandas as pd
from gensim import models
import dask.dataframe as dd
import dask.bag as db

model = models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False, no_header=True)
docs = dd.read_parquet("../output.pq/")
texts = docs['News_Tokens'].compute()
bag = db.from_sequence(texts)
sentences = bag.flatten()
words = sentences.map(lambda x: x.split()).flatten().distinct().compute()
results = []
threshold = 0.7
skipped=0
for word in words:
    try:
        # Получаем 10 ближайших слов
        similar_words = model.most_similar(word, topn=10)
        filtered_words = [(w, sim) for w, sim in similar_words if sim >= threshold]
        results.append([word, filtered_words])
    except KeyError:
        # Если слово не в модели, пропускаем его
        print(f"Слово '{word}' не найдено в модели.")
        skipped += 1
        continue

df_results = pd.DataFrame(results, columns=["Word", "Most_Similar_Word"])
print(f"Пропустили {skipped} слов")
df_results.to_csv("sim-words_sent_5_glove.csv", index=False)
\end{minted}
\subsection{Сравнение тезаурусов}
Возьмём случайные 10 слов и найдём самые близкие слова по мнению каждой модели. Ознакомиться с программным кодом, который выполняет это можно в приложении \ref{apx:cherrypick}. Результат 

Полученныый результат отражен на табл. \ref{table:cherrypick}.

\begin{table}[!h]
    \centering
    \footnotesize
    \begin{tabular}{|l|l|l|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Word}} & \multicolumn{1}{c|}{\textbf{GloVe}}                                                                                                                            & \textbf{Word2Vec}                                                                                                                                                    \\ \hline
    недорого                            & \begin{tabular}[c]{@{}l@{}}{[}('развлекаться', 0.77), ('метать', 0.73), \\ ('непогода', 0.72), ('энергично', 0.72), \\ ('географически', 0.71){]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}{[}('инк', 0.86), ('прожект', 0.86), \\ ('хармонь', 0.85), ('траффик', 0.85), \\ ('берингия', 0.85)"{]}\end{tabular}                      \\ \hline
    голод                               & {[}('холод', 0.72){]}                                                                                                                                          & \begin{tabular}[c]{@{}l@{}}{[}('фашист', 0.86), ('холод', 0.84), \\ ('умирать', 0.83), ('утрата', 0.83), \\ ('умерший', 0.83){]}\end{tabular}                        \\ \hline
    орфография                          & {[}('пунктуация', 0.86){]}                                                                                                                                     & {[}('пунктуация', 0.72){]}                                                                                                                                           \\ \hline
    внимательность                      & {[}('сосредоточенность', 0.74){]}                                                                                                                              & \begin{tabular}[c]{@{}l@{}}{[}('быстрота', 0.84), ('коммуникабельность', 0.78), \\ ('смекалка', 0.77), ('сноровка', 0.77), \\ ('находчивость', 0.76){]}\end{tabular} \\ \hline
    обсценный                           & \begin{tabular}[c]{@{}l@{}}{[}('нецензурный', 0.76), \\ ('лексика', 0.74){]}\end{tabular}                                                                      & \begin{tabular}[c]{@{}l@{}}{[}('жаргон', 0.80), ('фразеологический', 0.79), \\ ('иврит', 0.78), ('искра', 0.77), \\ ('онтогенетический', 0.77){]}\end{tabular}       \\ \hline
    трещина                             & {[}('ворона', 0.71){]}                                                                                                                                         & \begin{tabular}[c]{@{}l@{}}{[}('термометр', 0.90), ('макароны', 0.89), \\ ('укладка', 0.89), ('наледь', 0.89), \\ ('балка', 0.89){]}\end{tabular}                    \\ \hline
    кадмий                              & {[}('селенид', 0.86){]}                                                                                                                                        & \begin{tabular}[c]{@{}l@{}}{[}('селенид', 0.94), ('силанизировать', 0.87), \\ ('стабилизировать', 0.87), ('альбумин', 0.86),\\ ('бесшовный', 0.86){]}\end{tabular}   \\ \hline
    омс                                 & {[}('полис', 0.70){]}                                                                                                                                          & \begin{tabular}[c]{@{}l@{}}{[}('полис', 0.91), ('снилс', 0.86), \\ ('дубликат', 0.82), ('предъявление', 0.82), \\ ('пин', 0.81){]}\end{tabular}                      \\ \hline
    автомат                             & {[}('калашников', 0.74){]}                                                                                                                                     & \begin{tabular}[c]{@{}l@{}}{[}('калашников', 0.75), ('разборка', 0.73), \\ ('сборка', 0.72), ('гранат', 0.71){]}\end{tabular}                                        \\ \hline
    жажда                               & {[}('утолять', 0.84){]}                                                                                                                                        & \begin{tabular}[c]{@{}l@{}}{[}('сдерживать', 0.77), ('запах', 0.77), \\ ('наполнять', 0.77), ('притворство', 0.77), \\ ('аромат', 0.77){]}\end{tabular}              \\ \hline
    \end{tabular}
    \caption{Самых близких слов по мнению GloVe и Word2Vec для 10 случайных слов из корпуса}
    \label{table:cherrypick}
\end{table}

Видим, что в целом модель GloVe даёт более точные ответы. Кроме этого, модель Word2Vec предлагает сильно больше близких слов, большинство из которых ничего общего с рассматриваемым словом не имеет. Заметим, что существуют слова, для которых модель дает совсем неправильные ответы (слово \textit{недорого}, \textit{трещина}).

Сгенерируем для каждой модели граф. Граф является ориентированным. Вершиной в графе является слово, а ребро "--- факт того, что слово является близким для рассматриваемого с весом, равным косиносному расстоянию между ними. С программным кодом, генерирующим подобный граф, можно ознакомиться в приложении \ref{apx:graph}.

Характеристики графов отражены в табл. \ref{table:graph}.

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Характеристика}}                                                & \multicolumn{1}{c|}{\textbf{GloVe}} & \textbf{Word2Vec} \\ \hline
    Кол-во вершин                                                                                & 18402                               & 18401             \\ \hline
    Кол-во ребер                                                                                 & 9921                                & 86834             \\ \hline
    Кол-во неизолированных вершин                                                                & 3690                                & 12908             \\ \hline
    \begin{tabular}[c]{@{}l@{}}Кол-во компонент \\ (без учёта изолированных вершин)\end{tabular} & 770                                 & 337               \\ \hline
    Кол-во сообществ                                                                             & 800                                 & 349               \\ \hline
    Модулярность                                                                                 & 0.453                               & 0.597             \\ \hline
    \end{tabular}
    \caption{Сравнительная характеристика графов моделей}
    \label{table:graph}
\end{table}

Визуализируем полученные графы с помощью программы Gephi "--- бесплатный пакет программ для анализа и визуализации сетей. Ознакомиться с результатом можно на рисунках \ref{fig:graph-word2vec}, \ref{fig:graph-glove}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{word2vec.png}
    \caption{Граф, полученный моделью Word2Vec}
    \label{fig:graph-word2vec}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{glove.png}
    \caption{Граф, полученный моделью GloVe}
    \label{fig:graph-glove}
\end{figure}

\conclusion


% Отобразить все источники. Даже те, на которые нет ссылок.
% \nocite{*}
% \inputencoding{cp1251}
% \bibliographystyle{gost780uv}
% \bibliography{thesis}
% \inputencoding{utf8}

% Окончание основного документа и начало приложений Каждая последующая секция
% документа будет являться приложением
\appendix
\section{Программный код, выполняющий обработку текста}
\label{apx:tokenization}
\begin{minted}{python3}
import pandas as pd
import pymorphy3
import re
import dask.dataframe as dd

def string_dev(a_in):
    x_in = re.sub(r'\n', ' ', a_in)
    y_in = x_in.lower()
    b_in = re.sub(r'&\w+;', ' ', y_in)
    d_in = re.sub(r'<[^>]*>', ' ', b_in)
    f_in = re.sub(r'www\.\w+\.\w{2,3}?', ' ', d_in)
    a_in = re.sub(r'\xad', '', f_in)
    c_in = re.sub(r'\\xa0-', '', a_in)
    u_in = re.sub(r'\\u200e', '', c_in)
    w_in = re.sub(r'\d+', '', u_in)
    y_in = re.sub(r'\-', ' ', w_in)
    yy_in = re.sub(r'_+', ' ', y_in)
    yyy_in = re.sub(r'[…—–/()"\[\]\\\\,\-:;<>©@№#%\'\+\*“”&~\$\^«»_і‑]+', '', yy_in)
    q_in = re.sub(r'[a-z]*', '', yyy_in)
    sss_in = re.sub(r'\b\w{,2}\b', '', q_in)
    qms_in = re.sub(r'\s{2,}', ' ', sss_in)
    nms_in = qms_in.strip()
    return nms_in

def lematization(f_input_list):
    morph = pymorphy3.MorphAnalyzer()
    lnorm = []
    for word in f_input_list:
        p = morph.parse(word)[0]
        lnorm.append(p.normal_form)
    return lnorm

def del_my_stop_words(word_tokens):
    stop_words_nltk = {'который', 'кому', 'имя', 'сегодня', 'вчера', 'завтра', 'также', 'в', 'во', 'свой',
                       'это', 'часто', 'зачастую', 'мочь', 'смочь', 'а', 'без', 'более', 'больше', 'будет',
                       'будто', 'бы', 'был', 'была', 'были', 'было', 'быть', 'в', 'вам', 'вас', 'вдруг',
                       'ведь', 'во', 'вот', 'впрочем', 'все', 'всегда', 'всего', 'всех', 'всю', 'вы', 'где',
                       'да', 'даже', 'два', 'для', 'до', 'другой', 'его', 'ее', 'ей', 'ему', 'если', 'есть',
                       'еще', 'ж', 'же', 'за', 'зачем', 'здесь', 'и', 'из', 'или', 'им', 'иногда', 'их', 'к',
                       'как', 'какая', 'какой', 'когда', 'конечно', 'кто', 'куда', 'ли', 'лучше', 'между',
                       'меня', 'мне', 'много', 'может', 'можно', 'мой', 'моя', 'мы', 'на', 'над', 'надо',
                       'наконец', 'нас', 'не', 'него', 'нее', 'ней', 'нельзя', 'нет', 'ни', 'нибудь', 'никогда',
                       'ним', 'них', 'ничего', 'но', 'ну', 'о', 'об', 'один', 'он', 'она', 'они', 'опять', 'от',
                       'перед', 'по', 'под', 'после', 'потом', 'потому', 'почти', 'при', 'про', 'раз', 'разве',
                       'с', 'со', 'сам', 'свою', 'себе', 'себя', 'сейчас', 'c', 'со', 'совсем', 'так', 'такой',
                       'там', 'тебя', 'тем', 'теперь', 'то', 'тогда', 'того', 'тоже', 'только', 'том', 'тот',
                       'три', 'тут', 'ты', 'у', 'уж', 'уже', 'хорошо', 'хоть', 'что', 'чего', 'чем', 'через',
                       'что', 'чтоб', 'чтобы', 'чуть', 'эти', 'этого', 'этой', 'этом', 'этот', 'эту', 'я',
                       'сказал', 'человек', 'жизнь', 'говорил', 'кажется', 'сказать', 'сегодня', 'сказала',
                       'сказал'}

    my_stop_words = {'сгт', 'свой', 'стать', 'кроме', 'разный', 'около', 'затем', 'помимо', 'ваш', 'вам',
                     'некоторый', 'лишь', 'каждый', 'самый', 'также', 'неоднократно', 'ещё', 'сразу', 'среди',
                     'однако', 'вновь', 'иной', 'ныне', 'пока', 'хотя', 'либо', 'немного', 'гораздо', 'ничто',
                     'нередко', 'наоборот', 'впереди', 'таковой', 'мимо', 'тесно', 'вряд', 'нечто', 'почём',
                     'почему', 'любой', 'обратно', 'оттуда', 'очень', 'понапрасну', 'поскольку', 'поэтому',
                     'прежде', 'причём', 'прочий', 'пусть', 'наш', 'несколько', 'никак', 'твой', 'подробный',
                     'информация'}

    stop_words = stop_words_nltk | my_stop_words
    filtered_sentence = [w for w in word_tokens if w not in stop_words]
    return filtered_sentence

from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsNERTagger, Doc

segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
ner_tagger = NewsNERTagger(emb)

def remove_proper_nouns(text):
    if not isinstance(text, str) or not text.strip():
        print("text имеет неожидаемый тип")
        return "error type"
    try:
        doc = Doc(text)
        doc.segment(segmenter)
        doc.tag_morph(morph_tagger)
        doc.tag_ner(ner_tagger)
        if not doc.spans:
            return text
        spans_to_remove = [span for span in doc.spans if span.type in ['PER', 'LOC', 'ORG']]
        text_cleaned = text
        for span in sorted(spans_to_remove, key=lambda x: x.start, reverse=True):
            text_cleaned = text_cleaned[:span.start] + text_cleaned[span.stop:]
        print(text_cleaned)
        return text_cleaned.strip()
    except Exception as e:
        print(f"Ошибка при обработке текста: {text[:50]}... Ошибка: {str(e)}")
        return "error"

def process_str_of_the_news(string):
    string_2 = string_dev(string)
    if len(string_2) != 0:
        list_of_sentences = re.split(r'[.!?]', string_2)
        list_of_sentences = [x.split(' ') for x in list_of_sentences if x]
        filtered_sentences = []
        for sentence in list_of_sentences:
            no_stopwords_1 = del_my_stop_words(sentence)
            lemmatized = lematization(no_stopwords_1)
            text = ' '.join(lemmatized)
            text = re.sub(r'\b\w{1,2}\b', '', text)
            text = re.sub(r'\s{2,}', ' ', text)
            text = text.strip()
            if text:
                filtered_sentences.append(text)
    return filtered_sentences

import pyarrow as pa
def apply_tokenization():
    docs = dd.read_parquet("raw-data.pq").repartition(npartitions=8).loc[:1]
    print("Read finished")
    docs['News_Tokens'] = docs['News_Text'].map_partitions(
        lambda s: s.apply(remove_proper_nouns),
        meta=("News_Tokens", object)
    )
    docs['News_Tokens'] = docs["News_Tokens"].map_partitions(
        lambda s: s.apply(process_str_of_the_news),
        meta=("News_Tokens", object)
    )
    print("Returning")
    schema = {
        "News_Text": pa.string(),
        "News_Tokens": pa.list_(pa.string()),
        "News_Title": pa.string()
    }
    return docs.to_parquet("output.pq", schema=schema, write_metadata_file=True)

apply_tokenization()
\end{minted}
\section{Получение случайных 20 слов}
\label{apx:cherrypick}
\begin{minted}{python3}
import pandas as pd
import os

# Пути к файлам и соответствующие названия колонок
file_paths = {
    # 'bert': '/bert/sim-words5-bert.csv',
    'glove': '/glove_python/sim-words_sent_5_glove.csv',
    'word2vec': '/word2vec/similar_words_sent_5.csv'
}

# Создаем пустой DataFrame с колонкой 'word'
data = pd.DataFrame(columns=['word'])

# Обрабатываем каждый файл
for model_name, path in file_paths.items():
    # Читаем CSV-файл
    full_path = ".." + path
    df = pd.read_csv(full_path)
    
    # Переименовываем колонку Most_Similar_Word в название модели
    df = df.rename(columns={'Most_Similar_Word': model_name})
    
    # Если это первый файл, используем его слова как основу
    if data.empty:
        data['word'] = df['Word']
    
    # Добавляем данные в основной DataFrame
    data[model_name] = df[model_name]
import ast
# Функция для проверки, является ли значение пустым списком или NaN
def is_valid_similar_words(value):
    if pd.isna(value):  # проверяем NaN
        return False
    if isinstance(value, str):  # если данные хранятся как строки (например, "[('слово', 0.5), ...]")
        try:
            lst = ast.literal_eval(value)  # преобразуем строку в список
            return len(lst) > 0  # True, если список не пустой
        except (ValueError, SyntaxError):
            return False
    elif isinstance(value, list):  # если данные уже в формате списка
        return len(value) > 0
    else:
        return False  # на случай других форматов

# Применяем фильтрацию: оставляем строки, где ВСЕ столбцы с похожими словами не пусты
filtered_data = data[
    data['glove'].apply(is_valid_similar_words) &
    data['word2vec'].apply(is_valid_similar_words)
]
n = 25  # кол-во случайных строк

random_sample = filtered_data.sample(n=n, random_state=42)  # random_state для воспроизводимости

# Сохраняем в новый CSV-файл
output_file = 'random_sample_similar_words.csv'
random_sample.to_csv(output_file, index=False, encoding='utf-8')
\end{minted}

\section{Построение графа}
\label{apx:graph}
\begin{minted}{python3}
import pandas as pd
import os

# Пути к файлам и соответствующие названия колонок
file_paths = {
    # 'bert': '/bert/sim-words5-bert.csv',
    'glove': '/glove_python/sim-words_sent_5_glove.csv',
    'word2vec': '/word2vec/similar_words_sent_5.csv'
}


data = {}
for model_name, path in file_paths.items():
  full_path = ".." + path
  df = pd.read_csv(full_path)
  data[model_name] = df
import networkx as nx
from ast import literal_eval

graphs = {}
for model_name, df in data.items():
    G = nx.Graph()  # или nx.DiGraph() для направленного графа
    
    for _, row in df.iterrows():
        word = row['Word']
        similar_words = row['Most_Similar_Word']
        
        # Пропускаем пустые списки
        if not similar_words or pd.isna(similar_words):
            continue
        
        # Преобразуем строку в список кортежей (если данные в формате строки)
        if isinstance(similar_words, str):
            try:
                similar_words = literal_eval(similar_words)
            except (ValueError, SyntaxError):
                continue
        
        # Добавляем рёбра в граф
        for similar_word, weight in similar_words:
            G.add_edge(word, similar_word, weight=weight)
    graphs[model_name] = G    
    # nx.write_gexf(G, f"{model_name}_graph.gexf")  # формат GEXF для Gephi
    print(f"Граф '{model_name}' содержит {len(G.nodes)} узлов и {len(G.edges)} рёбер.")
from community import community_louvain
for model_name, G in graphs.items():
  print(f"model name: {model_name}")
  partition = community_louvain.best_partition(G, weight='weight')
  # Кол-во кластеров
  num_clusters = max(partition.values()) + 1
  print(f"Число кластеров (Louvain): {num_clusters}")
  
  # Размеры кластеров
  from collections import Counter
  cluster_sizes = Counter(partition.values())
  print(f"Размеры кластеров: {cluster_sizes.most_common(5)}")  # Топ-5 кластеров

  # 3. Модулярность (качество кластеризации)
  modularity = community_louvain.modularity(partition, G, weight='weight')
  print(f"Модулярность: {modularity:.3f}")
\end{minted}
\end{document}
  
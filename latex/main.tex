\documentclass[coursework]{SCWorks}
% Тип обучения (одно из значений):
%    bachelor   - бакалавриат (по умолчанию)
%    spec       - специальность
%    master     - магистратура
% Форма обучения (одно из значений):
%    och        - очное (по умолчанию)
%    zaoch      - заочное
% Тип работы (одно из значений):
%    coursework - курсовая работа (по умолчанию)
%    referat    - реферат
%  * otchet     - универсальный отчет
%  * nir%ournal - журнал НИР
%  * digital    - итоговая работа для цифровой кафдры
%    diploma    - дипломная работа
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% Включение шрифта
%    times      - включение шрифта Times New Roman (если установлен)
%                 по умолчанию выключен
\usepackage{preamble}
\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Построение и сравнение тезаурусов на основе анализа текстовых данных новостных источников}

% Курс
\course{3}

% Группа
\group{351}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
% \department{факультета КНиИТ}

% Специальность/направление код - наименование
% \napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
% \napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
% \napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
% \napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
% \studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Янченко Вадима Александровича}

% Заведующий кафедрой 
\chtitle{доцент, к.\,ф.-м.\,н.}
\chname{С.\,В.\,Миронов}

% Руководитель ДПП ПП для цифровой кафедры (перекрывает заведующего кафедры)
% \chpretitle{
%     заведующий кафедрой математических основ информатики и олимпиадного\\
%     программирования на базе МАОУ <<Ф"=Т лицей №1>>
% }
% \chtitle{г. Саратов, к.\,ф.-м.\,н., доцент}
% \chname{Кондратова\, Ю.\,Н.}

% Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Руководитель практики от организации (руководитель для цифровой кафедры)
\patitle{доцент, к.\,ф.-м.\,н.}
\paname{С.\,В.\,Миронов}

% Руководитель НИР
\nirtitle{доцент, к.\,п.\,н.} % степень, звание
\nirname{В.\,А.\,Векслер}

% Семестр (только для практики, для остальных типов работ не используется)
\term{2}

% Наименование практики (только для практики, для остальных типов работ не
% используется)
\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики, для
% остальных типов работ не используется)
\duration{2}

% Даты начала и окончания практики (только для практики, для остальных типов
% работ не используется)
\practStart{01.07.2022}
\practFinish{13.01.2023}

% Год выполнения отчета
\date{2025}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам (по умолчанию -
% нумерация сквозная) (допускается оба вида нумерации)
\secNumbering

\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
% \abbreviations
% \begin{description}
%     \item ... "--- ...
%     \item ... "--- ...
% \end{description}

% Раздел "Определения". Может отсутствовать в работе
% \definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и
% "Определения"
% \defabbr

\intro
В задачах обработки естественного языка (NLP) и информационного поиска (IR) важную роль играет использование различных видов знаний: лексических связей между словами, значений слов, специализированных понятий в предметных областях и знаний. Одним из традиционных способов представления такого рода знаний в системах NLP являются тезаурусы\cite{loukachevitch2021ruthes}.

В контексте компьютерной обработки текста тезаурус представляет собой формализованный ресурс, в котором описаны семантические связи между словами или терминами — например, синонимы, гипонимы и другие виды лексических отношений. Благодаря такой структуре, тезаурусы могут использоваться в автоматизированных системах для анализа, поиска и интерпретации текстовой информации.

Использование готовых тезаурусов, обученных на огромном количестве произведений, не всегда даёт лучший результат из"=за особенностей предметной области. Такие ресурсы, как правило, отражают общую лексику языка, но не учитывают контекстуальные особенности и терминологию конкретных тематик.

В связи с этим всё более актуальным становится автоматическое построение тезаурусов на основе анализа конкретных текстов. Такой подход позволяет выявить семантические связи, характерные именно для выбранного корпуса, что способствует более точному и релевантному представлению знаний.

Цель данной курсовой работы "--- построить и сравнить тезаурусы, созданные с помощью методов PMI, Word2Vec, GloVe и BERT, на основе корпуса новостных текстов. Это позволит оценить, насколько различаются семантические представления в зависимости от выбранной модели и насколько точно каждый подход отражает смысловые связи в реальных текстах.

Для достижения поставленной цели необходимо решить следующие задачи:
\begin{itemize}
  \item изучить методы PMI, Word2Vec, GloVe и BERT в построении тезаурусов;
  \item построить тезаурусы с использованием этих методов;
  \item провести сравнительный анализ полученных тезаурусов
\end{itemize}

\section{Понятие тезауруса}
В информационном поиске часто прибегают к механизму расширения поискового запроса. Заключается он в добавлении связанных терминов или понятий к исходному запросу, по сути, переписывая его. Это помогает найти документы, которые могут не совпадать с исходными ключевыми словами, но при этом удовлетворять информационные потребности пользователя.

Для поиска синонимов или близких по смыслу слов прибегают к использованию тезауруса. Тезаурус представляет собой структурированный лексический ресурс, в котором фиксируются семантические отношения между словами и терминами. В отличие от обычных словарей, тезаурусы явно отображают связи между ними — такие как синонимия, антонимия, родо-видовые отношения, ассоциативные связи и другие.

Современные тезаурусы могут быть как ручными (созданными экспертами в конкретной предметной области), так и автоматически построенными на основе анализа текстов с применением статистических и нейросетевых моделей. Автоматические тезаурусы особенно актуальны в условиях быстро меняющейся лексики, характерной, например, для новостных источников.

Одним из наиболее известных лексических ресурсов в сфере компьютерной 
лингвистики  и автоматической обработки текстов является компьютерный тезаурус WordNet. WordNet версии 3.0  включает приблизительно 155 тысяч различных лексем и словосочетаний, организованных в 117 тысяч понятий, или совокупностей синонимов (synset), общее число пар лексема – значение составляет более 200 тысяч.

Основным отношением в WordNet является отношение синонимии. Наборы 
синонимов "--- синсеты "--- являются основными структурными элементами WordNet.  
Понятие синонимии, используемое разработчиками WordNet, базируется на критерии, что два выражения являются синонимичными, если замена одного из них на другое в предложении не меняет значения истинности этого высказывания.

% https://istina.msu.ru/media/publications/book/171/c90/1283494/louk_book.pdf

При этом не требуется заменяемости синонимов во всех контекстах – по такому 
критерию в естественном языке было бы слишком мало синонимов. Используется 
значительно более слабое утверждение, что синонимы WordNet должны быть 
взаимозаменимы хотя бы в некотором множестве контекстов. Например, замена plank (доска, планка) для слова board (доска) редко меняет значение истинности в контексте плотницкого дела, но существуют контексты, где такая замена не может считаться приемлемой.

\section{Построение тезауруса}

Современные методы автоматического построения тезаурусов базируются на анализе больших текстовых корпусов и извлечении статистических или контекстуальных связей между словами. Основная идея таких подходов заключается в том, что слова, встречающиеся в схожих контекстах, как правило, имеют близкие значения. Возникает проблема отражения смысла слова.

Начнём с рассмотрения слова (возьмём, например, слово \textit{рама}). Слово \textit{рама} является леммой, то есть находится в начальной форме слова. Форма инфинитива используется в качестве леммы для глагола. Конкретные слова \textit{рамы} или \textit{мыли} являются словоформами.

Каждая лемма может иметь несколько значений. Тот факт, что леммы обладают полисемией (многозначностью) может затруднить интерпретацию. 

Кроме того, возможны случаи, когда одно слово имеет смысл, значение которого индентично смыслу другого слова или почти идентично. 
Такие слова называют синонимами. Примеры синонимов: \textit{кавалерия} "--- \textit{конница}, \textit{смелый} "---\textit{храбрый}.

Более формальное определение синонимии следующее: два слова являются синонимами, если они заменяют друг друга в любом предложении без изменения условий истинности предложения, то есть ситуаций, в которых предложение будет истинным.

На практике же два слова вероятно не обладают абсолютно идентичным смыслом. Слова могут различаться оттенками в значениях (\textit{мокрый} "--- \textit{влажный}) или употребляться в разных сферах (\textit{жена} (общеупотр.)"--- \textit{супруга} (офиц.)).

Хотя у слов не так много синонимов, у большинства слов есть много похожих слов. Кошка не является синонимом собаки, но кошки и собаки - это, безусловно, похожие слова. При переходе от синонимии к сходству будет полезно перейти от разговора об отношениях между смыслами слов (как при синонимии) к отношениям между словами (как при сходстве). Работа со словами позволяет избежать необходимости придерживаться определенного представления смыслов слов, что, как выясняется, упрощает нашу задачу. 

Понятие сходства слов очень полезно в больших семантических задачах. Знание того, насколько похожи два слова, может помочь в вычислении того, насколько схожи значения двух фраз или предложений.
% \subsection{Статистический метод. PMI}
% Одним из простейших и наиболее понятных методов извлечения семантических связей является использование метода поточечной взаимной информации (PMI). 

% Если говорить в общем, PMI - это метрика ассоциации, которая сравнивает относительную частоту двух исходов, происходящих вместе, с вероятностью того, что любой из них произойдет независимо.
\subsection{Word2Vec}
Ключевая идея состоит в представлении слова как вектора в многомерном пространстве. Самый простой способ перехода от слова к вектору состоит в следующем. Пусть $V$ "--- множество всех слов. Слово $w_i \in V$ представим как вектор размера $n = |V|$, в котором все координаты равны $0$ кроме $i$"=ой координаты, которая равна $1$.

Рассмотрим другой способ представление слова как вектора "--- эмбеддинг. Эмбеддинги имеет количество размерностей $d$ от 50 до 1000, что существенно меньше количества слов $|V|$ во всех документах. Кроме того, эти векторы плотные: вместо того, чтобы большинство координат были равны 0, значения будут вещественными числами, которые могут быть и отрицательными.

Эмбеддинги Word2Vec являются статическими, это означает, что каждому слову соответствует ровно один эмбеддинг. Такие модели как BERT позволяют создавать контекстуальные эмбеддинги, которые сопоставляют разный вектор к слову в зависимости от контекста.

Word2Vec реализуется в двух основных архитектурах: \textbf{CBOW} и \textbf{Skip"=Gram}.

\subsubsection{Архитектура модели Skip-Gram}

Модель Skip-Gram является одной из двух основных архитектур Word2Vec и направлена на предсказание контекстных слов по центральному слову. То есть, имея слово $w_t$, модель обучается предсказывать слова, находящиеся в некотором окне вокруг него: $w_{t-c}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+c}$, где $c$ — размер окна.


Модель Skip-Gram представляет собой простую нейросеть с одним скрытым слоем. Она состоит из следующих компонентов:

\begin{enumerate}
  \item \textbf{Входной слой.}  
  Каждое слово из словаря $V$ кодируется как one-hot вектор $\mathbf{x} \in \mathbb{R}^{|V|}$, в котором все элементы равны нулю, кроме одного, соответствующего индексу слова $w$.

  \item \textbf{Скрытый слой.}  
  Представлен матрицей весов $W \in \mathbb{R}^{|V| \times d}$, где $d$ — размерность эмбеддингового пространства. Преобразование входа:
  \[
  \mathbf{h} = \mathbf{x}^\top W
  \]
  Вектор $\mathbf{h} \in \mathbb{R}^d$ и является искомым эмбеддингом слова $w$.

  \item \textbf{Выходной слой.}  
  Представлен второй матрицей весов $W' \in \mathbb{R}^{d \times |V|}$. Результат выходного слоя:
  \[
  \mathbf{u} = W'^\top \mathbf{h}
  \]
  где $\mathbf{u} \in \mathbb{R}^{|V|}$ — логиты для каждого слова в словаре.

  \item \textbf{Softmax.}  
  На выходе применяется softmax-функция, преобразующая логиты в вероятности:
  \[
  P(w_O \mid w_I) = \frac{\exp(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I})}{\sum_{w \in V} \exp(\mathbf{v}'_w \cdot \mathbf{v}_{w_I})}
  \]
  Здесь:
  \begin{itemize}
    \item $w_I$ — входное (центральное) слово;
    \item $w_O$ — слово из контекста;
    \item $\mathbf{v}_{w_I}$ — вектор из матрицы $W$ (входной эмбеддинг);
    \item $\mathbf{v}'_{w_O}$ — вектор из матрицы $W'$ (выходной эмбеддинг).
  \end{itemize}
\end{enumerate}

Рассмотрим функцию потерь.
Для одного примера $(w_I, w_O)$ используется кросс-энтропия:
\[
\mathcal{L} = -\log P(w_O \mid w_I)
\]
Общая функция потерь по корпусу:
\[
\mathcal{L}_{\text{total}} = -\sum_{t=1}^{T} \sum_{\substack{-c \leq j \leq c \\ j \ne 0}} \log P(w_{t+j} \mid w_t)
\]
где $T$ — длина корпуса.

Эмбеддингом является как матрица ($W$), так и матрица($W'$) — часто используется только $W$. 

Что стоит отметить: хотя в модель не заложено явно никакой семантики, а только статистические свойства корпусов текстов, оказывается, что натренированная модель word2vec может улавливать некоторые семантические свойства слов. Классический пример: cлово <<мужчина>> относится к слову <<женщина>> так же, как слово <<дядя>> к слову <<тётя>>, что для нас совершенно естественно и понятно, но в других моделям добиться такого же соотношения векторов можно только с помощью специальных ухищрений. Здесь же — это происходит естественно из самого корпуса текстов.

\subsubsection{Negative sampling}
Полноценный расчёт softmax-функции в модели Skip-Gram требует вычисления скалярного произведения с каждым словом в словаре, что приводит к высокой вычислительной сложности: $\mathcal{O}(|V|)$ на каждый пример.

Для решения этой проблемы в модели Word2Vec используется приближённый метод оптимизации "--- negative sampling.

Вместо того чтобы обучать модель различать целевое слово $w_O$ от всех остальных слов в словаре $V$, negative sampling предлагает:
\begin{enumerate}
  \item обучать модель различать настоящие пары (центральное слово и слово из его контекста),
  \item и несколько случайных пар (центральное слово и случайные, нерелевантные слова), которые считаются негативными примерами.
\end{enumerate}

Пусть:
\begin{itemize}
  \item $w_I$ — центральное слово (input word),
  \item $w_O$ — контекстное слово (output word, положительный пример),
  \item $w_1^-, w_2^-, \dots, w_K^-$ — отрицательные примеры, выбранные случайным образом из словаря по заданному распределению.
\end{itemize}

Тогда оптимизируется следующая функция потерь:

\[
\mathcal{L} = \log \sigma(\mathbf{v}'_{w_O} \cdot \mathbf{v}_{w_I}) + \sum_{k=1}^{K} \mathbb{E}_{w_k^- \sim P_n(w)} \left[ \log \sigma(-\mathbf{v}'_{w_k^-} \cdot \mathbf{v}_{w_I}) \right]
\]

где:
\begin{itemize}
  \item $\sigma(x) = \frac{1}{1 + e^{-x}}$ — сигмоида;
  \item $\mathbf{v}_{w_I}$ — входной эмбеддинг слова $w_I$;
  \item $\mathbf{v}'_{w_O}$ — выходной эмбеддинг контекстного слова $w_O$;
  \item $P_n(w)$ — распределение, по которому выбираются отрицательные примеры;
  \item $K$ — число отрицательных примеров.
\end{itemize}

Для выбора отрицательных слов используется неравномерное распределение. По умолчанию (в оригинальной статье Word2Vec) это распределение частот слов в степени $3/4$:

\[
P_n(w) = \frac{f(w)^{3/4}}{\sum_{w' \in V} f(w')^{3/4}}
\]

где $f(w)$ — частота слова $w$ в корпусе.

% Вкратце Skip"=Gram заключается в следующем:
% \begin{enumerate}
%   \item Рассматривается центральное слово и окружающий его контекст. 
%   \item Ставится задача классификации
%   \item Количество классов "--- размер словаря $|V| = n$.
%   \item На вход нейросеть принимает слово, выдает $n$ значений "--- распределение на слова в словаре.
%   \item Функция потерь "--- кросc"=энтропия между распределением, выданным сетью, и верным распределением (one"=hot вектор)
% \end{enumerate}


  
\subsection{GloVe}
% GloVe (Global Vectors) — это другая модель построения эмбеддингов.

% Пусть матрица $X$ "--- матрица ко"=частотности (co-occurrence), где $X_{ij}$ показывает, сколько раз слово $j$ встречается в контексте слова $i$. Пусть $X_i = \sum_k X_{ik}$ "--- количество всех слов, встречающихся в контексте слова $i$.
% Пусть $P_{ij} = P(j | i) = X_{ij} / X_i$ будет вероятностью того, что слово $j$ встретилось в контексте слова $i$.

Модель GloVe (Global Vectors) предназначена для построения векторных представлений слов на основе глобальной статистики совместных появлений слов в тексте. В отличие от моделей Word2Vec, которые обучаются на локальных контекстных окнах, GloVe использует информацию о числе совместных появлений слов во всём корпусе, агрегируя её в специальную матрицу.

На первом этапе строится матрица совместных появлений $X \in \mathbb{R}^{|V| \times |V|}$, где $X_{ij}$ — количество раз, когда слово $j$ встречается в контексте слова $i$. Контекст обычно ограничивается окном фиксированного размера (например, 5 слов влево и вправо), при этом можно учитывать взвешивание по расстоянию до центрального слова.

GloVe обучает два векторных представления: $w_i \in \mathbb{R}^d$ — для центрального слова $i$ и $\tilde{w}_j \in \mathbb{R}^d$ — для контекстного слова $j$, а также два скалярных смещения $b_i$ и $\tilde{b}_j$. Обучение направлено на аппроксимацию следующего равенства:

\[
w_i^\top \tilde{w}_j + b_i + \tilde{b}_j \approx \log X_{ij}
\]

Целевая функция, которую минимизирует модель, записывается в следующем виде:

\[
J = \sum_{i=1}^{|V|} \sum_{j=1}^{|V|} f(X_{ij}) \left(w_i^\top \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij} \right)^2
\]

где $f(X_{ij})$ — весовая функция, ограничивающая влияние часто встречающихся пар слов, например:

\[
f(x) = 
\begin{cases}
\left(\frac{x}{x_{\text{max}}}\right)^\alpha & \text{если } x < x_{\text{max}} \\
1 & \text{иначе}
\end{cases}
\]

На практике используются значения $\alpha = 0{,}75$ и $x_{\text{max}} = 100$.

% Обучение проводится методом стохастического градиентного спуска или его модификаций (например, AdaGrad). На каждом шаге параметры $w_i$, $\tilde{w}_j$, $b_i$, $\tilde{b}_j$ обновляются таким образом, чтобы минимизировать ошибку между скалярным произведением векторов и логарифмом числа совместных появлений.

После завершения обучения итоговое представление слова $i$ получается как сумма обученных векторов:

\[
v_i = w_i + \tilde{w}_i
\]

Таким образом, каждая лексема кодируется плотным вектором фиксированной размерности, отражающим её глобальные статистические связи с другими словами корпуса.


% Word2Vec фокусируется на локальном контексте, обучаясь на парах центр-контекст, и опирается на информацию из небольших окон. В отличие от неё, GloVe использует глобальную статистику словоупотребления по всему корпусу, строя векторные представления на основе матрицы ко-частотности (co-occurrence matrix) — то есть того, насколько часто слова встречаются рядом друг с другом.

% Это позволяет GloVe более явно учитывать семантические связи между словами, которые могут не быть очевидными в локальных контекстах.

% Мб добавить + PMI
% \subsection{Контекстуальные модели. BERT}

% \section{Методы сравнения тезаурусов}

\section{Практическая часть}
В рамках практической части осуществляется построение тезауруса на основе новостей сайтов СГУ и СГТУ. В процессе построения используются описанные выше модели Word2Vec и GloVe.

Весь программный код написан на языке программирования python версии $3.12.6$. В программном коде используются следующие библиотеки:
\begin{itemize}
  \item pandas для удобного управления данными;
  \item gensim для работы с моделями
  % \item 
\end{itemize}
\subsection{Предварительная обработка данных}
Рассмотрим данные, которые представляют из себя новости, взятые из официального сайта СГУ. Каждый документ характеризуется следующими параметрами:
\begin{enumerate}
  \item дата написания новости;
  \item заголовок новости;
  \item необработанный текст новости;
  \item обработанный текст новости;
  \item количество слов;
  \item количество токенов.
\end{enumerate}

Обработка текста заключается в удалении стоп"=слов и лемматизации слов.

Стоп-слова — это слова, которые встречаются в языке чрезвычайно часто, но, как правило, не несут самостоятельной смысловой нагрузки в контексте информационного поиска или анализа текста. К таким словам относятся, например, союзы, предлоги, частицы, местоимения: «и», «в», «на», «что», «как», «это», и т.п. Их исключение позволяет снизить «шум» в текстах, повысить эффективность алгоритмов и сосредоточиться на лексемах, действительно значимых для понимания содержания документа.
  
Лемматизация, в свою очередь, представляет собой процесс приведения слова к его начальной, словарной форме — лемме. Это позволяет уменьшить количество уникальных словоформ в тексте, объединив различные грамматические формы одного и того же слова (например, «пошёл», «идёт», «шли» — к лемме «идти»). Благодаря лемматизации повышается точность анализа, поскольку слова с одинаковым значением, но разными формами, будут обрабатываться как единое понятие.

В новостях часто встречаются имена людей, городов, стран, наименования различных компаний, которые не привносят смысл в предложение. Избавимся от них тоже:
\begin{minted}{python3}
import pandas as pd
from natasha import (
      NewsEmbedding,
      NewsNERTagger,
      Doc
  )
docs = pd.read_excel("News_SGU_31077_Processed_1.xlsx")

emb = NewsEmbedding()
ner_tagger = NewsNERTagger(emb)

def remove_proper_nouns(text):
    if not isinstance(text, str) or not text.strip():
        return text
    
    doc = Doc(text)
    doc.tag_ner(ner_tagger)
    
    if not doc.spans:
        return text
        
    spans_to_remove = [span for span in doc.spans if span.type in ['PER', 'LOC', 'ORG']]
    
    if not spans_to_remove:
        return text
        
    text_cleaned = text
    for span in sorted(spans_to_remove, key=lambda x: x.start, reverse=True):
        text_cleaned = text_cleaned[:span.start] + text_cleaned[span.stop:]
        
    return text_cleaned.strip()
    
docs['News_Tokens'] = docs['News_Tokens'].apply(remove_proper_nouns)
docs.to_csv("data.csv", index=False)
\end{minted}

\subsection{Построение тезаурусов}
\subsubsection{Word2Vec}
Обучим модель Word2Vec на полученных данных. Для этого напишем следующую программу:
\begin{minted}{python3}
from gensim.models import Word2Vec
import pandas as pd

document = pd.read_csv("data.csv")
texts = document["News_Tokens"].apply(lambda x: x.split(" "))
model = Word2Vec(sentences=texts, vector_size=100, window=5,min_count=1, sg=0)
model.save('word2vec_5_sgu.model')
\end{minted}

Получив модель, построим тезаурус:
\begin{minted}{python3}
from gensim.models import Word2Vec
import pandas as pd

document = pd.read_csv("data.csv")
texts = document["News_Tokens"].apply(lambda x: x.split(" ")).explode().unique()
model = Word2Vec.load("word2vec_5_sgu.model")

results = []

for word in texts:
    try:
        # Получаем 10 ближайших слов
        similar_words = model.wv.most_similar(word, topn=10)
        results.append([word, similar_words])
    except KeyError:
        # Если слово не в модели, пропускаем его
        print(f"Слово '{word}' не найдено в модели.")

df_results = pd.DataFrame(results, columns=["Word", "Most_Similar_Word"])

df_results.to_csv("similar_words10.csv", index=False)
\end{minted}
\subsubsection{GloVe}
Для создания модели GloVe склонируем их официальный репозиторий\footnote{https://github.com/stanfordnlp/GloVe}. Далее выполним следующие команды:
\begin{minted}{bash}
build/vocab_count -min-count 5 -verbose 2 < corpus.txt > vocab.txt
build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 15 < corpustxt >cooccurrence.bin
build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin
build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max10 -iter 15 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2
\end{minted}

Далее сгенерируем модель:
\begin{minted}{python3}
import pandas as pd
from gensim import models

model = models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False, no_header=True)
document = pd.read_excel("../News_SGU_31077_Processed_1.xlsx")
texts = document["News_Tokens"].apply(lambda x: x.split(" ")).explode().unique()
results = []
skipped=0
for word in texts:
    try:
        # Получаем 10 ближайших слов
        similar_words = model.most_similar(word, topn=10)
        results.append([word, similar_words])
    except KeyError:
        # Если слово не в модели, пропускаем его
        skipped += 1
        continue

df_results = pd.DataFrame(results, columns=["Word", "Most_Similar_Word"])
print(f"Пропустили {skipped} слов")
df_results.to_csv("sim-words10-glove.csv", index=False)
\end{minted}
\subsection{Сравнение тезаурусов}
Возьмём случайные 20 слов и найдём самые близкие слова по мнению каждой модели.

Для этого напишем скрипт:
\begin{minted}{python3}
import pandas as pd
import os

# Пути к файлам и соответствующие названия колонок
file_paths = {
    'bert': '/bert/sim-words5-bert.csv',
    'glove': '/glove_python/sim-words10-glove.csv',
    'pmi': '/pmi/similar_words_pmi.csv',
    'word2vec': '/word2vec/similar_words10.csv'
}

# Создаем пустой DataFrame с колонкой 'word'
data = pd.DataFrame(columns=['word'])

# Обрабатываем каждый файл
for model_name, path in file_paths.items():
    # Читаем CSV-файл
    full_path = ".." + path
    df = pd.read_csv(full_path)
    
    # Переименовываем колонку Most_Similar_Word в название модели
    df = df.rename(columns={'Most_Similar_Word': model_name})
    
    # Если это первый файл, используем его слова как основу
    if data.empty:
        data['word'] = df['Word']
    
    # Добавляем данные в основной DataFrame
    data[model_name] = df[model_name]
import ast
# Функция для проверки, является ли значение пустым спискили NaN
def is_valid_similar_words(value):
    if pd.isna(value):  # проверяем NaN
        return False
    if isinstance(value, str):  # если данные хранятся кстроки (например, "[('слово', 0.5), ...]")
        try:
            lst = ast.literal_eval(value)  # преобразустроку в список
            return len(lst) > 0  # True, если список пустой
        except (ValueError, SyntaxError):
            return False
    elif isinstance(value, list):  # если данные ужеформате списка
        return len(value) > 0
    else:
        return False  # на случай других форматов

# Применяем фильтрацию: оставляем строки, где ВСЕ столбцыпохожими словами не пусты
filtered_data = data[
    data['bert'].apply(is_valid_similar_words) &
    data['glove'].apply(is_valid_similar_words) &
    data['pmi'].apply(is_valid_similar_words) &
    data['word2vec'].apply(is_valid_similar_words)
]
n = 100  # кол-во лучайных стр
random_sample = filtered_data.sample(n=n, random_state=42) # random_state для воспроизводимости

# Сохраняем в новый CSV-файл
output_file = 'random_sample_similar_words.csv'
random_sample.to_csv(output_file, index=Falsencoding='utf-8')
\end{minted}

Полученныый результат отражен на табл. \ref{table:cherrypick}.

Сгенерируем для каждой модели граф.
\begin{minted}{python3}
import pandas as pd
import os

# Пути к файлам и соответствующие названия колонок
file_paths = {
    'bert': '/bert/sim-words5-bert.csv',
    'glove': '/glove_python/sim-words10-glove.csv',
    'pmi': '/pmi/similar_words_pmi.csv',
    'word2vec': '/word2vec/similar_words10.csv'
}

data = {}
for model_name, path in file_paths.items():
  full_path = ".." + path
  df = pd.read_csv(full_path)
  data[model_name] = df
import networkx as nx
from ast import literal_eval

graphs = {}
for model_name, df in data.items():
    G = nx.Graph()  # или nx.DiGraph() для направленного графа
    
    for _, row in df.iterrows():
        word = row['Word']
        similar_words = row['Most_Similar_Word']
        
        # Пропускаем пустые списки
        if not similar_words or pd.isna(similar_words):
            continue
        
        # Преобразуем строку в список кортежей (если данные в формате строки)
        if isinstance(similar_words, str):
            try:
                similar_words = literal_eval(similar_words)
            except (ValueError, SyntaxError):
                continue
        
        # Добавляем рёбра в граф
        for similar_word, weight in similar_words:
            G.add_edge(word, similar_word, weight=weight)
    graphs[model_name] = G    
    # nx.write_gexf(G, f"{model_name}_graph.gexf")  # форматEXF для Gephi
    print(f"Граф '{model_name}' содержит {len(G.nodes)} узлов {len(G.edges)} рёбер.")

from community import community_louvain
for model_name, G in graphs.items():
  print(f"model name: {model_name}")
  partition = community_louvain.best_partition(G, weight='weight')
  # Кол-во кластеров
  num_clusters = max(partition.values()) + 1
  print(f"Число кластеров (Louvain): {num_clusters}")
  
  # Размеры кластеров
  from collections import Counter
  cluster_sizes = Counter(partition.values())
  print(f"Размеры кластеров: {cluster_sizes.most_common(5)}")  # Топ-5 кластеров

  # 3. Модулярность (качество кластеризации)
  modularity = community_louvain.modularity(partition, G, weight='weight')
  print(f"Модулярность: {modularity:.3f}")
\end{minted}

Результат отражен в табл. \ref{table:graph}.

Визуализируем полученные графы с помощью программы Gephi "--- бесплатный пакет программ для анализа и визуализации сетей. Ознакомиться с результатом можно на рисунках \ref{fig:graph-word2vec}, \ref{fig:graph-glove}.


\conclusion


% Отобразить все источники. Даже те, на которые нет ссылок.
% \nocite{*}
% \inputencoding{cp1251}
% \bibliographystyle{gost780uv}
% \bibliography{thesis}
% \inputencoding{utf8}

% Окончание основного документа и начало приложений Каждая последующая секция
% документа будет являться приложением
\appendix

\end{document}
  
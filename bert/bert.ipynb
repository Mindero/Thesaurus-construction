{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import scipy\n",
    "import math  \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = 'ai-forever/ruBert-base'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "document = pd.read_excel(\"../News_SGU_31077_Processed_1.xlsx\")\n",
    "texts = document[\"News_Tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mean embeddings: 100%|██████████| 3/3 [00:00<00:00, 305.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "word_embedding = defaultdict(list)\n",
    "texts = texts[:1]\n",
    "\n",
    "\n",
    "max_length = 512  # Максимальная длина для модели\n",
    "stride = 128      # Перекрытие между чанками для контекста\n",
    "\n",
    "for text in tqdm(texts, desc=\"get embeddings\"):\n",
    "    # Токенизация с перекрытием (stride)\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    # Удаляем лишние поля, которые модель не принимает\n",
    "    chunk_inputs = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"token_type_ids\": inputs.get(\"token_type_ids\")  # Опционально, если есть\n",
    "    }\n",
    "    # print(text)\n",
    "    # Обрабатываем каждый чанк отдельно\n",
    "    for i in range(inputs[\"input_ids\"].shape[0]):\n",
    "        # Подготавливаем входы для текущего чанка\n",
    "        current_input = {\n",
    "            k: v[i].unsqueeze(0) for k, v in chunk_inputs.items() if v is not None\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**current_input)\n",
    "        \n",
    "        embeddings = outputs.last_hidden_state[0].numpy()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][i])\n",
    "        offsets = inputs[\"offset_mapping\"][i]\n",
    "        \n",
    "        # Собираем эмбеддинги только для первых вхождений токенов\n",
    "        for j, (token, offset) in enumerate(zip(tokens, offsets)):\n",
    "            if offset[0] == 0 and not token.startswith(\"##\"):\n",
    "                # print(token)\n",
    "                word_embedding[token].append(embeddings[j])\n",
    "\n",
    "print(len(word_embedding))\n",
    "# print(word_embedding)\n",
    "# Усреднение эмбеддингов для каждого слова\n",
    "for word in tqdm(word_embedding, desc=\"mean embeddings\"):\n",
    "    word_embedding[word] = np.mean(word_embedding[word], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word_embedding.keys())\n",
    "print(words[:10])\n",
    "print(word_embedding[words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.array([word_embedding[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar_words(word, top_n=5):\n",
    "    if word not in word_embedding:\n",
    "        return []\n",
    "    idx = words.index(word)\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_words = [(words[i], score) for i, score in sim_scores[1:top_n+1]]  # [1:] чтобы исключить само слово\n",
    "    return top_words\n",
    "\n",
    "print(get_top_similar_words(\"сгу\"))\n",
    "# Пример вывода: [(\"large\", 0.92), (\"huge\", 0.89), (\"enormous\", 0.85), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for word in tqdm(words, desc=\"get_sim_word\"):\n",
    "  result.append([word, get_top_similar_words(word)])\n",
    "df_result = pd.DataFrame(result, columns=[\"Word\", \"Most_Similar_Word\"])\n",
    "df_result.to_csv(\"sim-words5-bert.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vadim\\scoop\\apps\\python\\3.12.6\\Lib\\site-packages\\IPython\\core\\magics\\pylab.py:166: UserWarning: pylab import has clobbered these variables: ['text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import scipy\n",
    "import math  \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = 'ai-forever/ruBert-base'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "document = pd.read_excel(\"../News_SGU_31077_Processed_1.xlsx\")\n",
    "texts = document[\"News_Tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31077\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get embeddings: 100%|██████████| 31077/31077 [3:34:58<00:00,  2.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71796\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "word_embedding = defaultdict(list)\n",
    "# texts = texts[:1000]\n",
    "\n",
    "max_length = 512\n",
    "stride = 256\n",
    "\n",
    "for text in tqdm(texts, desc=\"get embeddings\"):\n",
    "    # Токенизация с word_ids для отслеживания исходных слов\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    chunk_inputs = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"token_type_ids\": inputs.get(\"token_type_ids\"),\n",
    "    }\n",
    "\n",
    "    # Обрабатываем каждый чанк\n",
    "    for i in range(inputs[\"input_ids\"].shape[0]):\n",
    "        current_input = {\n",
    "            k: v[i].unsqueeze(0) for k, v in chunk_inputs.items() if v is not None\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**current_input)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state[0].numpy()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][i])\n",
    "        word_ids = inputs.word_ids(batch_index=i)  # Получаем индексы слов для каждого токена\n",
    "\n",
    "        # Группируем эмбеддинги по словам\n",
    "        current_id = None\n",
    "        current_word = \"\"\n",
    "        word_embeddings_temp = []\n",
    "        for j, (token, word_id) in enumerate(zip(tokens, word_ids)):\n",
    "            if word_id is not None: # Не служебный символ\n",
    "                if word_id != current_id:\n",
    "                    # Если началось новое слово, сохраняем предыдущее (если было)\n",
    "                    if current_id is not None:\n",
    "                        word_embedding[current_word].append(np.mean(word_embeddings_temp, axis=0))\n",
    "                    current_id = word_id\n",
    "                    current_word = token\n",
    "                    word_embeddings_temp = [embeddings[j]]\n",
    "                else:\n",
    "                    # Добавляем эмбеддинг подтокена к текущему слову\n",
    "                    current_word += token[2:]\n",
    "                    word_embeddings_temp.append(embeddings[j])\n",
    "\n",
    "        # Добавляем последнее слово в чанке\n",
    "        if current_id is not None:\n",
    "            word_embedding[current_word].append(np.mean(word_embeddings_temp, axis=0))\n",
    "\n",
    "# Усредняем эмбеддинги для каждого слова (если слово встречается в нескольких чанках)\n",
    "for word in word_embedding:\n",
    "    word_embedding[word] = np.mean(word_embedding[word], axis=0)\n",
    "# Теперь word_embedding содержит эмбеддинги целых слов (не подтокенов)\n",
    "print(len(word_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48035258  0.75337803  0.37968978 ...  0.53577435  0.89375055\n",
      "   0.09614167]\n",
      " [ 0.9589678   0.11851829 -0.09660891 ...  0.20498708 -0.13616273\n",
      "   0.46041542]\n",
      " [ 0.17332472 -0.24964423  0.1084326  ... -0.19036618 -0.6807928\n",
      "   0.46571916]\n",
      " ...\n",
      " [-0.29190627  0.09819216  0.24448213 ... -0.06546737  0.06231318\n",
      "   0.55827415]\n",
      " [ 0.03648516 -0.04748931  0.8381751  ... -0.16265407  0.08596082\n",
      "  -0.05247702]\n",
      " [ 0.35626978  0.40936068 -0.37038222 ...  0.23060995  0.28282765\n",
      "   0.16283448]]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word_embedding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.array([word_embedding[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('биф', 0.84560966), ('сгуоб', 0.8114157)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_similar_words(word, top_n=10, threshold = 0.6):\n",
    "    if word not in word_embedding:\n",
    "        return []\n",
    "    idx = words.index(word)\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_words = [(words[i], score) for i, score in sim_scores[1:top_n+1] if score > threshold]  # [1:] чтобы исключить само слово\n",
    "    return top_words\n",
    "\n",
    "print(get_top_similar_words(\"сгу\"))\n",
    "# Пример вывода: [(\"large\", 0.92), (\"huge\", 0.89), (\"enormous\", 0.85), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_sim_word: 100%|██████████| 71796/71796 [3:01:55<00:00,  6.58it/s]  \n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for word in tqdm(words, desc=\"get_sim_word\"):\n",
    "  result.append([word, get_top_similar_words(word)])\n",
    "df_result = pd.DataFrame(result, columns=[\"Word\", \"Most_Similar_Word\"])\n",
    "df_result.to_csv(\"sim-words5-bert.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

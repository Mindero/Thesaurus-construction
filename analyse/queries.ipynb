{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "def load_index_and_ids(prefix: str, folder: str = \"news-embeddings\"):\n",
    "    # Загрузка FAISS индекса\n",
    "    index = faiss.read_index(f\"{folder}/{prefix}.index\")\n",
    "    \n",
    "    # Загрузка соответствующих ID\n",
    "    with open(f\"{folder}/{prefix}_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        ids = json.load(f)\n",
    "    \n",
    "    return index, ids\n",
    "db = {\n",
    "  \"glove\": load_index_and_ids(\"glove\"),\n",
    "  \"word2vec\": load_index_and_ids(\"word2vec\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "models = {\n",
    "  \"glove\": KeyedVectors.load(\"../glove_python/glove.kv\"),\n",
    "  \"word2vec\": KeyedVectors.load(\"../word2vec/word2vec.kv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считывание новостей\n",
    "import dask.dataframe as dd\n",
    "docs = dd.read_parquet(\"../output.pq/\", columns=['News_Id', 'News_Title', 'News_Tokens'])\n",
    "df = docs.compute()\n",
    "id_to_title = dict(zip(df['News_Id'], df['News_Title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "import re\n",
    "def lematization(f_input_list):          # Лематизация слов в списке\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lnorm = list()\n",
    "    for word in f_input_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lnorm.append(p.normal_form)\n",
    "    return (lnorm)\n",
    "def preprocess_query(query):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r\"[^\\w\\s]\", \" \", query)\n",
    "    query = re.sub(r'\\s{2,}', ' ', query)\n",
    "    tokens = query.split()\n",
    "    return lematization(tokens)\n",
    "def get_query_embedding(query: str, model_name: str):\n",
    "    model: KeyedVectors = models[model_name]\n",
    "    tokens = preprocess_query(query=query)\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if not vectors:\n",
    "        print(\"Query vector is empty\")\n",
    "        return np.zeros((1, model.vector_size), dtype='float32')\n",
    "    return np.mean(vectors, axis=0).astype('float32').reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, model_name: str, k = 5):\n",
    "    query_vec = get_query_embedding(query, model_name)\n",
    "    if np.linalg.norm(query_vec) == 0:\n",
    "        return []\n",
    "    index, ids = db[model_name]\n",
    "    D, I = index.search(query_vec, k)\n",
    "    news_ids = [ids[i] for i in I[0]]\n",
    "    return news_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Чемпионат мира по программированию\"\n",
    "print(f\"Запрос: {text[:50]}\\n\")\n",
    "answers = semantic_search(query=text, model_name=\"word2vec\")\n",
    "answers = [id_to_title[i] for i in answers]\n",
    "for answer in answers:\n",
    "  print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей через тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель выдает новость по запросу в топ K новостей\n",
    "def is_news_at_k_most(query: str, expected_id: int, model_name: str, k: int) -> bool:\n",
    "  news_id = semantic_search(query=query, model_name=model_name, k=k)\n",
    "  return any(id == expected_id for id in news_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as bag\n",
    "\n",
    "test_data = dd.read_csv(\"query-combined.csv/*.part\").loc[:1]\n",
    "\n",
    "def safe_parse(queries_str):\n",
    "    try:\n",
    "        return ast.literal_eval(queries_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "test_data = test_data.assign(News_Query_Parsed=test_data['News_Query'].map(safe_parse, meta=('News_Query_Parsed', 'object')))\n",
    "  \n",
    "def count_true_partition(df, model_name, k):\n",
    "    def count_row(row):\n",
    "        queries = row['News_Query_Parsed']\n",
    "        return sum(is_news_at_k_most(q, row['News_Id'], model_name, k) for q in queries)\n",
    "    df['count_true'] = df.apply(count_row, axis=1)\n",
    "    return df\n",
    "\n",
    "k = 5\n",
    "all_count = len(test_data)\n",
    "\n",
    "meta = test_data._meta.assign(count_true = 0)\n",
    "for model_name in models.keys():\n",
    "    result = test_data.map_partitions(count_true_partition, model_name, k, meta=meta).persist()\n",
    "    \n",
    "    result_df.to_csv(f\"{model_name}_test_data.csv\", single_file=True)\n",
    "\n",
    "    count = result.sum().compute()\n",
    "    print(f\"model = {model_name} k = {k} count = {count} percent = {count / all_count * 100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

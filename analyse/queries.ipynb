{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "def load_index_and_ids(prefix: str, folder: str = \"news-embeddings\"):\n",
    "    # Загрузка FAISS индекса\n",
    "    index = faiss.read_index(f\"{folder}/{prefix}.index\")\n",
    "    \n",
    "    # Загрузка соответствующих ID\n",
    "    with open(f\"{folder}/{prefix}_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        ids = json.load(f)\n",
    "    \n",
    "    return index, ids\n",
    "db = {\n",
    "  \"glove\": load_index_and_ids(\"glove\"),\n",
    "  \"word2vec\": load_index_and_ids(\"word2vec\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "models = {\n",
    "  \"glove\": KeyedVectors.load(\"../glove_python/glove.kv\"),\n",
    "  \"word2vec\": KeyedVectors.load(\"../word2vec/word2vec.kv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считывание новостей\n",
    "import dask.dataframe as dd\n",
    "docs = dd.read_parquet(\"../output.pq/\", columns=['News_Id', 'News_Title', 'News_Tokens'])\n",
    "df = docs.compute()\n",
    "id_to_title = dict(zip(df['News_Id'], df['News_Title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "import re\n",
    "def lematization(f_input_list):          # Лематизация слов в списке\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lnorm = list()\n",
    "    for word in f_input_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lnorm.append(p.normal_form)\n",
    "    return (lnorm)\n",
    "def preprocess_query(query):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r\"[^\\w\\s]\", \" \", query)\n",
    "    query = re.sub(r'\\s{2,}', ' ', query)\n",
    "    tokens = query.split()\n",
    "    return lematization(tokens)\n",
    "def get_query_embedding(query: str, model_name: str):\n",
    "    model: KeyedVectors = models[model_name]\n",
    "    tokens = preprocess_query(query=query)\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if not vectors:\n",
    "        print(\"Query vector is empty\")\n",
    "        return np.zeros((1, model.vector_size), dtype='float32')\n",
    "    return np.mean(vectors, axis=0).astype('float32').reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, model_name: str, k = 5):\n",
    "    query_vec = get_query_embedding(query, model_name)\n",
    "    if np.linalg.norm(query_vec) == 0:\n",
    "        return []\n",
    "    index, ids = db[model_name]\n",
    "    D, I = index.search(query_vec, k)\n",
    "    news_ids = [ids[i] for i in I[0]]\n",
    "    return news_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Чемпионат мира по программированию\n",
      "\n",
      "СГУ участвует в проведении финала чемпионата ACM-ICPC 2013\n",
      "Студент ИФКиС стал чемпионом России по кикбоксингу\n",
      "Программисты СГУ отправились на финал Чемпионата мира\n",
      "Профессор В.Н. Чинилов стал призёром Чемпионата России\n",
      "В Саратове пройдёт Неделя информатики\n"
     ]
    }
   ],
   "source": [
    "text = \"Чемпионат мира по программированию\"\n",
    "print(f\"Запрос: {text[:50]}\\n\")\n",
    "answers = semantic_search(query=text, model_name=\"word2vec\")\n",
    "answers = [id_to_title[i] for i in answers]\n",
    "for answer in answers:\n",
    "  print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей через тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель выдает новость по запросу в топ K новостей\n",
    "def is_news_at_k_most(query: str, expected_id: int, model_name: str, k: int) -> bool:\n",
    "  news_id = semantic_search(query=query, model_name=model_name, k=k)\n",
    "  return any(id == expected_id for id in news_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column assignment doesn't support type <class 'dask.dataframe.dask_expr._collection.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models.keys():\n\u001b[32m     27\u001b[39m     result = test_data.map_partitions(count_true_partition, model_name, k, meta=meta).persist()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     result_df = \u001b[43mtest_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount_true\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     result_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_test_data.csv\u001b[39m\u001b[33m\"\u001b[39m, single_file=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     32\u001b[39m     count = result.sum().compute()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:2831\u001b[39m, in \u001b[36mDataFrame.assign\u001b[39m\u001b[34m(self, **pairs)\u001b[39m\n\u001b[32m   2829\u001b[39m         v = from_dask_array(v, index=result.index, meta=result._meta)\n\u001b[32m   2830\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2831\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn assignment doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2832\u001b[39m     args.extend([k, v])\n\u001b[32m   2834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: Column assignment doesn't support type <class 'dask.dataframe.dask_expr._collection.DataFrame'>"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as bag\n",
    "\n",
    "test_data = dd.read_csv(\"query-combined.csv/*.part\").loc[:1]\n",
    "\n",
    "def safe_parse(queries_str):\n",
    "    try:\n",
    "        return ast.literal_eval(queries_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "test_data = test_data.assign(News_Query_Parsed=test_data['News_Query'].map(safe_parse, meta=('News_Query_Parsed', 'object')))\n",
    "  \n",
    "def count_true_partition(df, model_name, k):\n",
    "    def count_row(row):\n",
    "        queries = row['News_Query_Parsed']\n",
    "        return sum(is_news_at_k_most(q, row['News_Id'], model_name, k) for q in queries)\n",
    "    df['count_true'] = df.apply(count_row, axis=1)\n",
    "    return df\n",
    "\n",
    "k = 5\n",
    "all_count = len(test_data)\n",
    "\n",
    "meta = test_data._meta.assign(count_true = 0)\n",
    "for model_name in models.keys():\n",
    "    result = test_data.map_partitions(count_true_partition, model_name, k, meta=meta).persist()\n",
    "    \n",
    "    result_df.to_csv(f\"{model_name}_test_data.csv\", single_file=True)\n",
    "\n",
    "    count = result.sum().compute()\n",
    "    print(f\"model = {model_name} k = {k} count = {count} percent = {count / all_count * 100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

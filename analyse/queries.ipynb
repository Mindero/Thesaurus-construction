{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "def load_index_and_ids(prefix: str, folder: str = \"news-embeddings\"):\n",
    "    # Загрузка FAISS индекса\n",
    "    index = faiss.read_index(f\"{folder}/{prefix}.index\")\n",
    "    \n",
    "    # Загрузка соответствующих ID\n",
    "    with open(f\"{folder}/{prefix}_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        ids = json.load(f)\n",
    "    \n",
    "    return index, ids\n",
    "db = {\n",
    "  \"glove\": load_index_and_ids(\"glove\"),\n",
    "  \"word2vec\": load_index_and_ids(\"word2vec\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "models = {\n",
    "  \"glove\": KeyedVectors.load(\"../glove_python/glove.kv\"),\n",
    "  \"word2vec\": KeyedVectors.load(\"../word2vec/word2vec.kv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считывание новостей\n",
    "import dask.dataframe as dd\n",
    "docs = dd.read_parquet(\"../output.pq/\", columns=['News_Id', 'News_Title', 'News_Tokens'])\n",
    "df = docs.compute()\n",
    "id_to_title = dict(zip(df['News_Id'], df['News_Title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "import re\n",
    "def lematization(f_input_list):          # Лематизация слов в списке\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lnorm = list()\n",
    "    for word in f_input_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lnorm.append(p.normal_form)\n",
    "    return (lnorm)\n",
    "def preprocess_query(query):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r\"[^\\w\\s]\", \" \", query)\n",
    "    query = re.sub(r'\\s{2,}', ' ', query)\n",
    "    tokens = query.split()\n",
    "    return lematization(tokens)\n",
    "def get_query_embedding(query: str, model_name: str):\n",
    "    model: KeyedVectors = models[model_name]\n",
    "    tokens = preprocess_query(query=query)\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if not vectors:\n",
    "        print(\"Query vector is empty\")\n",
    "        return np.zeros((1, model.vector_size), dtype='float32')\n",
    "    return np.mean(vectors, axis=0).astype('float32').reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, model_name: str, k = 5):\n",
    "    query_vec = get_query_embedding(query, model_name)\n",
    "    if np.linalg.norm(query_vec) == 0:\n",
    "        return []\n",
    "    index, ids = db[model_name]\n",
    "    D, I = index.search(query_vec, k)\n",
    "    news_ids = [ids[i] for i in I[0]]\n",
    "    return news_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Чемпионат мира по программированию\n",
      "\n",
      "СГУ участвует в проведении финала чемпионата ACM-ICPC 2013\n",
      "Студент ИФКиС стал чемпионом России по кикбоксингу\n",
      "Программисты СГУ отправились на финал Чемпионата мира\n",
      "Профессор В.Н. Чинилов стал призёром Чемпионата России\n",
      "В Саратове пройдёт Неделя информатики\n"
     ]
    }
   ],
   "source": [
    "text = \"Чемпионат мира по программированию\"\n",
    "print(f\"Запрос: {text[:50]}\\n\")\n",
    "answers = semantic_search(query=text, model_name=\"word2vec\")\n",
    "answers = [id_to_title[i] for i in answers]\n",
    "for answer in answers:\n",
    "  print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей через тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель выдает новость по запросу в топ K новостей\n",
    "def is_news_at_k_most(query: str, expected_id: int, model_name: str, k: int) -> bool:\n",
    "  news_id = semantic_search(query=query, model_name=model_name, k=k)\n",
    "  return any(id == expected_id for id in news_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m all_count = \u001b[38;5;28mlen\u001b[39m(test_data)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models.keys():\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     result = \u001b[43mtest_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount_true_partition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcount_true\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mint\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     result_df = test_data.assign(count_true=result)\n\u001b[32m     28\u001b[39m     result_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_test_data.csv\u001b[39m\u001b[33m\"\u001b[39m, single_file=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vadim\\Desktop\\6 sem\\Курсовая\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:460\u001b[39m, in \u001b[36mFrameBase.persist\u001b[39m\u001b[34m(self, fuse, **kwargs)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpersist\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m    459\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.optimize(fuse=fuse)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vadim\\Desktop\\6 sem\\Курсовая\\.venv\\Lib\\site-packages\\dask\\base.py:346\u001b[39m, in \u001b[36mDaskMethodsMixin.persist\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpersist\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Persist this dask collection into memory\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into a Dask collection with the same\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    344\u001b[39m \u001b[33;03m    dask.persist\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     (result,) = \u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vadim\\Desktop\\6 sem\\Курсовая\\.venv\\Lib\\site-packages\\dask\\base.py:1021\u001b[39m, in \u001b[36mpersist\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, *args, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m     postpersists.append((rebuild, a_keys, state))\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m d = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(keys, results))\n\u001b[32m   1024\u001b[39m results2 = [r({k: d[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ks}, *s) \u001b[38;5;28;01mfor\u001b[39;00m r, ks, s \u001b[38;5;129;01min\u001b[39;00m postpersists]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\scoop\\apps\\python\\3.12.6\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\scoop\\apps\\python\\3.12.6\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as bag\n",
    "\n",
    "test_data = dd.read_csv(\"query-combined.csv/*.part\")\n",
    "\n",
    "def safe_parse(queries_str):\n",
    "    try:\n",
    "        return ast.literal_eval(queries_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "test_data = test_data.assign(News_Query_Parsed=test_data['News_Query'].map(safe_parse, meta=('News_Query_Parsed', 'object')))\n",
    "  \n",
    "def count_true_partition(df, model_name, k):\n",
    "    def count_row(row):\n",
    "        queries = row['News_Query_Parsed']\n",
    "        return sum(is_news_at_k_most(q, row['News_Id'], model_name, k) for q in queries)\n",
    "    return df.apply(count_row, axis=1)\n",
    "\n",
    "k = 5\n",
    "all_count = len(test_data)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    result = test_data.map_partitions(count_true_partition, model_name, k, meta=('count_true', 'int')).persist()\n",
    "    \n",
    "    result_df = test_data.assign(count_true=result)\n",
    "    result_df.to_csv(f\"{model_name}_test_data.csv\", single_file=True)\n",
    "\n",
    "    count = result.sum().compute()\n",
    "    print(f\"model = {model_name} k = {k} count = {count} percent = {count / all_count * 100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#\n",
    "# Пути к файлам и соответствующие названия колонок\n",
    "file_paths = {\n",
    "    'bert': '/bert/sim-words5-bert.csv',\n",
    "    'glove': '/glove_python/sim-words10-glove.csv',\n",
    "    'pmi': '/pmi/similar_words_pmi.csv',\n",
    "    'word2vec': '/word2vec/similar_words10.csv'\n",
    "}\n",
    "\n",
    "# Создаем пустой DataFrame с колонкой 'word'\n",
    "data = {}\n",
    "\n",
    "# Обрабатываем каждый файл\n",
    "for model_name, path in file_paths.items():\n",
    "    # Читаем CSV-файл\n",
    "    full_path = \"..\" + path\n",
    "    df = pd.read_csv(full_path)\n",
    "    \n",
    "    # Добавляем данные в основной DataFrame\n",
    "    data[model_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# Загрузка эталонной модели\n",
    "thesaurus = KeyedVectors.load_word2vec_format(\"thesaurus.w2v\", binary=True, unicode_errors='ignore')\n",
    "# thesaurus = Word2Vec.load(\"thesaurus.w2v\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm  # для прогресс-бара\n",
    "\n",
    "def compare_with_thesaurus(model_words, model_similarities, thesaurus, topn=10):\n",
    "    \"\"\"\n",
    "    Сравнивает модель с тезаурусом\n",
    "    \n",
    "    Args:\n",
    "        model_words: список слов из модели (ваши 'Word' столбцы)\n",
    "        model_similarities: соответствующие списки похожих слов и весов\n",
    "        thesaurus: эталонная модель\n",
    "        topn: сколько топовых слов сравнивать\n",
    "    \n",
    "    Returns:\n",
    "        dict: метрики сравнения\n",
    "    \"\"\"\n",
    "    total_similarity = 0\n",
    "    found_words = 0\n",
    "    valid_pairs = 0\n",
    "    \n",
    "    for word, sim_data in tqdm(zip(model_words, model_similarities), total=len(model_words)):\n",
    "        if word not in thesaurus:\n",
    "            print(word)\n",
    "            continue\n",
    "            \n",
    "        found_words += 1\n",
    "        model_top = [w for w, _ in sim_data[:topn]] if sim_data else []\n",
    "        \n",
    "        try:\n",
    "            thesaurus_top = [w for w, _ in thesaurus.most_similar(word, topn=topn)]\n",
    "            \n",
    "            # Вычисляем пересечение\n",
    "            common = set(model_top) & set(thesaurus_top)\n",
    "            total_similarity += len(common) / topn\n",
    "            valid_pairs += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    metrics = {\n",
    "        'coverage': found_words / len(model_words),\n",
    "        'avg_overlap': total_similarity / valid_pairs if valid_pairs else 0,\n",
    "        'total_words': len(model_words),\n",
    "        'matched_words': found_words,\n",
    "        'valid_pairs': valid_pairs\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for (model_name, df) in data.items():\n",
    "    print(f\"\\nProcessing {model_name}...\")\n",
    "    \n",
    "    # Преобразование строк в списки (если нужно)\n",
    "    df['Most_Similar_Word'] = df['Most_Similar_Word'].apply(\n",
    "        lambda x: eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    # Фильтрация пустых значений\n",
    "    valid_data = df.dropna(subset=['Most_Similar_Word'])\n",
    "    valid_data = valid_data[valid_data['Most_Similar_Word'].apply(len) > 0]\n",
    "    \n",
    "    # Сравнение с тезаурусом\n",
    "    metrics = compare_with_thesaurus(\n",
    "        valid_data['Word'].tolist(),\n",
    "        valid_data['Most_Similar_Word'].tolist(),\n",
    "        thesaurus,\n",
    "        topn=10\n",
    "    )\n",
    "    \n",
    "    results[model_name] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# График покрытия\n",
    "plt.figure(figsize=(10, 5))\n",
    "results_df['coverage'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Покрытие слов тезауруса')\n",
    "plt.ylabel('Доля совпадающих слов')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# График среднего пересечения\n",
    "plt.figure(figsize=(10, 5))\n",
    "results_df['avg_overlap'].plot(kind='bar', color='lightgreen')\n",
    "plt.title('Среднее пересечение топ-10 похожих слов')\n",
    "plt.ylabel('Средняя доля совпадений')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединение запросов от LLM и простых запросов (заголовок + весь текст)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "llm_query = dd.read_csv(\"../llm-query-csv/*.part\")\n",
    "simple_query = dd.read_parquet(\"query.pq\")\n",
    "\n",
    "merged = simple_query.merge(\n",
    "    llm_query,\n",
    "    on=['News_Id', 'News_Title', 'News_Text'],\n",
    "    how='left',\n",
    "    suffixes=('_simple', '_llm')\n",
    ")\n",
    "\n",
    "def combine_queries(row):\n",
    "    if isinstance(row['News_Query_llm'], list):\n",
    "        return row['News_Query_simple'] + row['News_Query_llm']\n",
    "    else:\n",
    "        return row['News_Query_simple']\n",
    "\n",
    "merged['News_Query'] = merged.apply(\n",
    "    combine_queries,\n",
    "    axis=1,\n",
    "    meta=('News_Query', 'object')\n",
    ")\n",
    "\n",
    "result = merged[['News_Id', 'News_Title', 'News_Text', 'News_Query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result))\n",
    "print(len(simple_query))\n",
    "print(len(llm_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"query-combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

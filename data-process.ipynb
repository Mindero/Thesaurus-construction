{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy3\n",
    "import re\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_dev (a_in):                              # Очистка текта от тегов и специальных символов\n",
    "    x_in = re.sub(r'\\n',' ',a_in)                    # удаление символов конца строки\n",
    "    y_in = x_in.lower()                             # приведение ситоки к нижнему регистру\n",
    "    b_in = re.sub(r'&\\w+;',' ', y_in)                 # замена тегов типа'&nbsp;'  на пробел\n",
    "    d_in = re.sub(r'<[^>]*>',' ', b_in)               # удаление тегов\n",
    "    f_in = re.sub(r'www.\\w+.\\w{2,3}?',' ', d_in)      # удаление адресов веб-серверов\n",
    "#\\xad\n",
    "#–\\xa0\n",
    "    a_in = re.sub(r'\\xad','', f_in)\n",
    "    c_in = re.sub(r'\\\\xa0-','', a_in)\n",
    "    u_in = re.sub(r'\\\\u200e','', c_in)    \n",
    "    w_in = re.sub(r'\\d+','', u_in)                        # удаление последовательностей цифр\n",
    "    y_in = re.sub(r'[\\-]',' ',w_in)\n",
    "    yy_in = re.sub(r'[_]+', ' ',y_in)\n",
    "    yyy_in = re.sub(r'[ι,…,—,–,//,\\(,\\),\",\\[,\\],\\\\\\\\,\\\\,,\\-,:,;,<,>,=,©,@,№,#,%,\\',\\+,\\*,“,”,&,∙,~,\\$,\\^,•,«,»,_,ι,і,‑,‘,’,і]+','',yy_in)\n",
    "#    s_in = re.sub(r'\\W', ' ', y_in)\n",
    "    q_in = re.sub(r'[a-z]*','', yyy_in)                 # удаление латинских букв\n",
    "    \n",
    "    sss_in = re.sub(r'\\b\\w{,2}\\b','', q_in)          # удаление всех слов длиной до 2-х букв\n",
    "    qms_in = re.sub(r'\\s{2,}',' ', q_in)            # замена кратного числа пробелов на один\n",
    "    nms_in = qms_in.strip()\n",
    "#    nms_in = re.sub(r'^[^\\w]*\\s', '', qms_in)         # удаление пробелов в начале строки\n",
    "#    mms_in = re.sub(r'\\s*$','', nms_in)              # удаление пробелов в конце строки\n",
    "    return (nms_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization (f_input_list):          # Лематизация слов в списке\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lnorm = list()\n",
    "    for word in f_input_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lnorm.append(p.normal_form)\n",
    "    return (lnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_my_stop_words(word_tokens): # удаление из списка моих стоп-слов\n",
    "    stop_words_nltk = {'который', 'кому', 'имя', 'сегодня', 'вчера', 'завтра', 'также', 'в', 'во', 'свой', \n",
    "                  'это', 'часто', 'зачастую', 'мочь', 'смочь','а', 'без', 'более', 'больше', 'будет', 'будто', \n",
    "                  'бы', 'был', 'была', 'были', 'было', 'быть', 'в', 'вам', 'вас', 'вдруг', 'ведь', 'во', 'вот', \n",
    "                  'впрочем', 'все', 'всегда', 'всего', 'всех', 'всю', 'вы', 'где', 'да', 'даже', 'два', 'для', \n",
    "                  'до', 'другой', 'его', 'ее', 'ей', 'ему', 'если', 'есть', 'еще', 'ж', 'же', 'за', 'зачем', 'здесь', \n",
    "                  'и', 'из', 'или', 'им', 'иногда', 'их', 'к', 'как', 'какая', 'какой', 'когда', 'конечно', 'кто', \n",
    "                  'куда', 'ли', 'лучше', 'между', 'меня', 'мне', 'много', 'может', 'можно', 'мой', 'моя', 'мы', 'на',\n",
    "                  'над', 'надо', 'наконец', 'нас', 'не', 'него', 'нее', 'ней', 'нельзя', 'нет', 'ни', 'нибудь', \n",
    "                  'никогда', 'ним', 'них', 'ничего', 'но', 'ну', 'о', 'об', 'один', 'он', 'она', 'они', 'опять', \n",
    "                  'от', 'перед', 'по', 'под', 'после', 'потом', 'потому', 'почти', 'при', 'про', 'раз', 'разве', \n",
    "                  'с', 'со', 'сам', 'свою', 'себе', 'себя', 'сейчас', 'c', 'со', 'совсем', 'так', 'такой', 'там', 'тебя', \n",
    "                  'тем', 'теперь', 'то', 'тогда', 'того', 'тоже', 'только', 'том', 'тот', 'три', 'тут', 'ты', \n",
    "                  'у', 'уж', 'уже', 'хорошо', 'хоть', 'что', 'чего', 'чем', 'через', 'что', 'чтоб', 'чтобы', 'чуть', \n",
    "                  'эти', 'этого', 'этой', 'этом', 'этот', 'эту', 'я', 'сказал', 'человек', 'жизнь', 'говорил', 'кажется', \n",
    "                  'сказать', 'сегодня', 'сказала', 'сказал'} \n",
    "\n",
    "    my_stop_words = {'сгт','свой', 'стать', 'кроме', 'разный', 'около', 'затем', 'помимо', 'ваш', 'вам', 'некоторый', \n",
    "                     'лишь', 'каждый', 'самый', 'также', 'неоднократно', 'ещё', 'сразу', 'среди',\n",
    "                  'однако', 'вновь', 'иной', 'ныне', 'пока', 'хотя','либо','немного', 'гораздо', 'ничто', 'нередко', 'ныне', \n",
    "                  'наоборот', 'впереди', 'таковой', 'мимо', 'тесно', 'вряд', 'нечто', 'почём', 'почему', 'любой', 'обратно',\n",
    "                  'оттуда', 'очень', 'понапрасну', 'поскольку', 'почему', 'поэтому', 'прежде', 'причём', 'прочий', 'пусть', \n",
    "                  'пока', 'это', 'наш', 'несколько', 'около', 'помимо', 'однако', 'сколько', 'либо', 'гораздо', 'ничто',\n",
    "                   'наоборот', 'никак', 'таковой', 'твой', 'нечто', 'понапрасну', 'почём', 'подробный', 'информация'}\n",
    "\n",
    "    stop_words = stop_words_nltk | my_stop_words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "\n",
    "def remove_proper_nouns(text):\n",
    "    # Пропускаем не-строки и пустые строки\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        print(\"text имеет неожидаемый тип\")\n",
    "        return \"error type\"\n",
    "    \n",
    "    try:\n",
    "        doc = Doc(text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        \n",
    "        # Если нет сущностей, возвращаем исходный текст\n",
    "        if not doc.spans:\n",
    "            return text\n",
    "        \n",
    "        # Собираем сущности для удаления\n",
    "        spans_to_remove = [span for span in doc.spans if span.type in ['PER', 'LOC', 'ORG']]\n",
    "        \n",
    "        # Если нечего удалять\n",
    "        if not spans_to_remove:\n",
    "            return text\n",
    "            \n",
    "        # Удаляем сущности (начиная с конца)\n",
    "        text_cleaned = text\n",
    "        for span in sorted(spans_to_remove, key=lambda x: x.start, reverse=True):\n",
    "            text_cleaned = text_cleaned[:span.start] + text_cleaned[span.stop:]\n",
    "        print(text_cleaned)\n",
    "        return text_cleaned.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке текста: {text[:50]}... Ошибка: {str(e)}\")\n",
    "        return \"error\"  # В случае ошибки возвращаем исходный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_str_of_the_news(string):  # Обработка сырой строки-новости \n",
    "    global abbr_list\n",
    "    string_2 = string_dev(string)     # Предобработка строки\n",
    "    \n",
    "    if len(string_2) != 0:  # Если строка непустая после обработки\n",
    "        list_of_sentences = re.split(r'[.!?]', string_2)\n",
    "        list_of_sentences = [x.split(' ') for x in list_of_sentences if x]\n",
    "        \n",
    "        filtered_sentences = []\n",
    "        for sentence in list_of_sentences:\n",
    "            no_stopwords_1 = del_my_stop_words(sentence)\n",
    "            lemmatized = lematization(no_stopwords_1)\n",
    "            text = ' '.join(lemmatized)\n",
    "            # Удаление слов длиной ≤ 2 символов\n",
    "            text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "            # Замена нескольких пробелов на один\n",
    "            text = re.sub(r'\\s{2,}', ' ', text)\n",
    "            text = text.strip()\n",
    "            if text and len(text) > 0:\n",
    "                filtered_sentences.append(text)\n",
    "    \n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read finished\n",
      "Returning\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "def apply_tokenization():\n",
    "    docs: dd.DataFrame = dd.read_parquet(\"raw-data.pq\").repartition(npartitions=8).loc[:1]\n",
    "    print(\"Read finished\")\n",
    "    docs['News_Tokens'] = docs['News_Text'].map_partitions(\n",
    "        lambda s: s.apply(remove_proper_nouns),\n",
    "        meta=(\"News_Tokens\", object)\n",
    "    )\n",
    "    docs['News_Tokens'] = docs[\"News_Tokens\"].map_partitions(lambda s: s.apply(process_str_of_the_news), meta=(\"News_Tokens\", object))\n",
    "    print(\"Returning\")\n",
    "    schema = {\n",
    "        \"News_Text\": pa.string(),\n",
    "        \"News_Tokens\": pa.list_(pa.string()),\n",
    "        \"News_Title\": pa.string()\n",
    "    }\n",
    "    return docs.to_parquet(\"output.pq\", schema=schema, write_metadata_file=True)\n",
    "apply_tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 мая состоялась общенародная акция «Бессмертный полк», в которой приняли участие студенты и сотрудники . После двух лет ограничений из-за пандемии долгожданное шествие проходило в традиционном формате. Во время построения колонны у X корпуса   задавал настроение концертной программой. Звучали песни военных лет, исполненные студентами и участниками  . Зрители, узнавая горячо любимые песни, подпевали и дарили свои аплодисменты. Вокальные номера перемежались с танцевальными. Ансамбль современной хореографии  «» представил выступления, в которых была отражена многонациональность и культурное многообразие . На пересечении улиц  и  колонна «» начала своё движение под мелодичные звуки оркестра. Участники акции бережно несли портреты своих родственников, которые приближали Победу на поле боя и в тылу, портреты павших героев Великой Отечественной войны, прадедов и дедов, отвоевавших мир для следующих поколений. Люди делились своей радостью и пели песни – «Смуглянка», «», «Синий платочек», воодушевлённо кричали «Ура!». В шествии приняли участие ректор  , проректоры, сотрудники и преподаватели, студенты .  отметил особую энергетику «Бессмертного полка»: «Идея акции пришла не сверху, а снизу, то есть она полностью создана на добровольных началах. Каждый приходит по своей воле и делится своими эмоциями. Посмотрите на лица участников шествия – все радостные, и всё потому, что они помнят. А если мы помним, то и наши ушедшие родственники живы в наших сердцах, и в этот день они вместе с нами». По словам , о шествии «» узнают и российские солдаты, которые сейчас принимают участие в военной спецоперации, он уверен, что акция поднимет их боевой дух. Шествие традиционно завершилось на , где горожан встречала концертная программа «Свет Великой Победы!». Участие в концерте приняли и артисты . Всего по данным организаторов в этом году на «Бессмертный полк» в  пришли более 120 тысяч человек.\n",
      "0    9 мая состоялась общенародная акция «Бессмертн...\n",
      "Name: News_Tokens, dtype: string\n"
     ]
    }
   ],
   "source": [
    "docs = dd.read_parquet(\"output.pq/\")\n",
    "value = docs['News_Tokens'].loc[0].compute()\n",
    "for x in value:\n",
    "    print(x)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.to_csv(\"data_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['News_Tokens'] = docs['News_Tokens'].apply(\n",
    "    lambda sentence_list: [remove_proper_nouns(sent) for sent in sentence_list]\n",
    ")\n",
    "docs.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_excel(\"News_SGU_31077_Processed_1.xlsx\")\n",
    "docs.to_parquet(\"raw-data.pq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

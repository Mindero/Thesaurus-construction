{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy3\n",
    "import re\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_dev (a_in):                              # Очистка текта от тегов и специальных символов\n",
    "    x_in = re.sub(r'\\n',' ',a_in)                    # удаление символов конца строки\n",
    "    y_in = x_in.lower()                             # приведение ситоки к нижнему регистру\n",
    "    b_in = re.sub(r'&\\w+;',' ', y_in)                 # замена тегов типа'&nbsp;'  на пробел\n",
    "    d_in = re.sub(r'<[^>]*>',' ', b_in)               # удаление тегов\n",
    "    f_in = re.sub(r'www.\\w+.\\w{2,3}?',' ', d_in)      # удаление адресов веб-серверов\n",
    "#\\xad\n",
    "#–\\xa0\n",
    "    a_in = re.sub(r'\\xad','', f_in)\n",
    "    c_in = re.sub(r'\\\\xa0-','', a_in)\n",
    "    u_in = re.sub(r'\\\\u200e','', c_in)    \n",
    "    w_in = re.sub(r'\\d+','', u_in)                        # удаление последовательностей цифр\n",
    "    y_in = re.sub(r'[\\-]',' ',w_in)\n",
    "    yy_in = re.sub(r'[_]+', ' ',y_in)\n",
    "    yyy_in = re.sub(r'[ι,…,—,–,//,\\(,\\),\",\\[,\\],\\\\\\\\,\\\\,,\\-,:,;,<,>,=,©,@,№,#,%,\\',\\+,\\*,“,”,&,∙,~,\\$,\\^,•,«,»,_,ι,і,‑,‘,’,і]+','',yy_in)\n",
    "#    s_in = re.sub(r'\\W', ' ', y_in)\n",
    "    q_in = re.sub(r'[a-z]*','', yyy_in)                 # удаление латинских букв\n",
    "    \n",
    "    sss_in = re.sub(r'\\b\\w{,2}\\b','', q_in)          # удаление всех слов длиной до 2-х букв\n",
    "    qms_in = re.sub(r'\\s{2,}',' ', q_in)            # замена кратного числа пробелов на один\n",
    "    nms_in = qms_in.strip()\n",
    "#    nms_in = re.sub(r'^[^\\w]*\\s', '', qms_in)         # удаление пробелов в начале строки\n",
    "#    mms_in = re.sub(r'\\s*$','', nms_in)              # удаление пробелов в конце строки\n",
    "    return (nms_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization (f_input_list):          # Лематизация слов в списке\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lnorm = list()\n",
    "    for word in f_input_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lnorm.append(p.normal_form)\n",
    "    return (lnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_my_stop_words(word_tokens): # удаление из списка моих стоп-слов\n",
    "    stop_words_nltk = {'который', 'кому', 'имя', 'сегодня', 'вчера', 'завтра', 'также', 'в', 'во', 'свой', \n",
    "                  'это', 'часто', 'зачастую', 'мочь', 'смочь','а', 'без', 'более', 'больше', 'будет', 'будто', \n",
    "                  'бы', 'был', 'была', 'были', 'было', 'быть', 'в', 'вам', 'вас', 'вдруг', 'ведь', 'во', 'вот', \n",
    "                  'впрочем', 'все', 'всегда', 'всего', 'всех', 'всю', 'вы', 'где', 'да', 'даже', 'два', 'для', \n",
    "                  'до', 'другой', 'его', 'ее', 'ей', 'ему', 'если', 'есть', 'еще', 'ж', 'же', 'за', 'зачем', 'здесь', \n",
    "                  'и', 'из', 'или', 'им', 'иногда', 'их', 'к', 'как', 'какая', 'какой', 'когда', 'конечно', 'кто', \n",
    "                  'куда', 'ли', 'лучше', 'между', 'меня', 'мне', 'много', 'может', 'можно', 'мой', 'моя', 'мы', 'на',\n",
    "                  'над', 'надо', 'наконец', 'нас', 'не', 'него', 'нее', 'ней', 'нельзя', 'нет', 'ни', 'нибудь', \n",
    "                  'никогда', 'ним', 'них', 'ничего', 'но', 'ну', 'о', 'об', 'один', 'он', 'она', 'они', 'опять', \n",
    "                  'от', 'перед', 'по', 'под', 'после', 'потом', 'потому', 'почти', 'при', 'про', 'раз', 'разве', \n",
    "                  'с', 'со', 'сам', 'свою', 'себе', 'себя', 'сейчас', 'c', 'со', 'совсем', 'так', 'такой', 'там', 'тебя', \n",
    "                  'тем', 'теперь', 'то', 'тогда', 'того', 'тоже', 'только', 'том', 'тот', 'три', 'тут', 'ты', \n",
    "                  'у', 'уж', 'уже', 'хорошо', 'хоть', 'что', 'чего', 'чем', 'через', 'что', 'чтоб', 'чтобы', 'чуть', \n",
    "                  'эти', 'этого', 'этой', 'этом', 'этот', 'эту', 'я', 'сказал', 'человек', 'жизнь', 'говорил', 'кажется', \n",
    "                  'сказать', 'сегодня', 'сказала', 'сказал'} \n",
    "\n",
    "    my_stop_words = {'сгт','свой', 'стать', 'кроме', 'разный', 'около', 'затем', 'помимо', 'ваш', 'вам', 'некоторый', \n",
    "                     'лишь', 'каждый', 'самый', 'также', 'неоднократно', 'ещё', 'сразу', 'среди',\n",
    "                  'однако', 'вновь', 'иной', 'ныне', 'пока', 'хотя','либо','немного', 'гораздо', 'ничто', 'нередко', 'ныне', \n",
    "                  'наоборот', 'впереди', 'таковой', 'мимо', 'тесно', 'вряд', 'нечто', 'почём', 'почему', 'любой', 'обратно',\n",
    "                  'оттуда', 'очень', 'понапрасну', 'поскольку', 'почему', 'поэтому', 'прежде', 'причём', 'прочий', 'пусть', \n",
    "                  'пока', 'это', 'наш', 'несколько', 'около', 'помимо', 'однако', 'сколько', 'либо', 'гораздо', 'ничто',\n",
    "                   'наоборот', 'никак', 'таковой', 'твой', 'нечто', 'понапрасну', 'почём', 'подробный', 'информация'}\n",
    "\n",
    "    stop_words = stop_words_nltk | my_stop_words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "\n",
    "def remove_proper_nouns(text):\n",
    "    # Пропускаем не-строки и пустые строки\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        print(\"text имеет неожидаемый тип\")\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        doc = Doc(text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        \n",
    "        # Если нет сущностей, возвращаем исходный текст\n",
    "        if not doc.spans:\n",
    "            return text\n",
    "        \n",
    "        # Собираем сущности для удаления\n",
    "        spans_to_remove = [span for span in doc.spans if span.type in ['PER', 'LOC', 'ORG']]\n",
    "        \n",
    "        # Если нечего удалять\n",
    "        if not spans_to_remove:\n",
    "            return text\n",
    "            \n",
    "        # Удаляем сущности (начиная с конца)\n",
    "        text_cleaned = text\n",
    "        for span in sorted(spans_to_remove, key=lambda x: x.start, reverse=True):\n",
    "            text_cleaned = text_cleaned[:span.start] + text_cleaned[span.stop:]\n",
    "            \n",
    "        return text_cleaned.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке текста: {text[:50]}... Ошибка: {str(e)}\")\n",
    "        return text  # В случае ошибки возвращаем исходный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_str_of_the_news(string):  # Обработка сырой строки-новости \n",
    "    global abbr_list\n",
    "    string_2 = string_dev(string)     # Предобработка строки\n",
    "    \n",
    "    if len(string_2) != 0:  # Если строка непустая после обработки\n",
    "        list_of_sentences = re.split(r'[.!?]', string_2)\n",
    "        list_of_sentences = [x.split(' ') for x in list_of_sentences if x]\n",
    "        \n",
    "        filtered_sentences = []\n",
    "        for sentence in list_of_sentences:\n",
    "            no_stopwords_1 = del_my_stop_words(sentence)\n",
    "            lemmatized = lematization(no_stopwords_1)\n",
    "            no_stopwords_2 = del_my_stop_words(lemmatized)\n",
    "            text = ' '.join(no_stopwords_2)\n",
    "            # Удаление слов длиной ≤ 2 символов\n",
    "            text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "            # Замена нескольких пробелов на один\n",
    "            text = re.sub(r'\\s{2,}', ' ', text)\n",
    "            text = text.strip()\n",
    "            if text and len(text) > 0:\n",
    "                filtered_sentences.append(text)\n",
    "    \n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\bodo\\transforms\\distributed_analysis.py:305: BodoWarning: No parallelism found for function 'apply_tokenization'. Distributed diagnostics:\n",
      "Distributed analysis set ''docs'' as replicated due to call to function 'pandas.read_excel' (unsupported function or usage)\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_9588\\3947763556.py\", line 3:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def apply_tokenization():\n",
    "    docs = pd.read_parquet(\"raw-data.pq\")\n",
    "    docs['News_Tokens'] = docs[\"News_Text\"].swifter.apply(process_str_of_the_news)\n",
    "    return docs\n",
    "docs = apply_tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.to_csv(\"data_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['News_Tokens'] = docs['News_Tokens'].apply(\n",
    "    lambda sentence_list: [remove_proper_nouns(sent) for sent in sentence_list]\n",
    ")\n",
    "docs.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_excel(\"News_SGU_31077_Processed_1.xlsx\")\n",
    "docs.to_parquet(\"raw-data.pq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy3\n",
    "import re\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_dev (a_in):                              # Очистка текта от тегов и специальных символов\n",
    "    x_in = re.sub(r'\\n',' ',a_in)                    # удаление символов конца строки\n",
    "    y_in = x_in.lower()                             # приведение ситоки к нижнему регистру\n",
    "    b_in = re.sub(r'&\\w+;',' ', y_in)                 # замена тегов типа'&nbsp;'  на пробел\n",
    "    d_in = re.sub(r'<[^>]*>',' ', b_in)               # удаление тегов\n",
    "    f_in = re.sub(r'www.\\w+.\\w{2,3}?',' ', d_in)      # удаление адресов веб-серверов\n",
    "#\\xad\n",
    "#–\\xa0\n",
    "    a_in = re.sub(r'\\xad','', f_in)\n",
    "    c_in = re.sub(r'\\\\xa0-','', a_in)\n",
    "    u_in = re.sub(r'\\\\u200e','', c_in)    \n",
    "    w_in = re.sub(r'\\d+','', u_in)                        # удаление последовательностей цифр\n",
    "    y_in = re.sub(r'[\\-]',' ',w_in)\n",
    "    yy_in = re.sub(r'[_]+', ' ',y_in)\n",
    "    yyy_in = re.sub(r'[ι,…,—,–,//,\\(,\\),\",\\[,\\],\\\\\\\\,\\\\,,\\-,:,;,<,>,=,©,@,№,#,%,\\',\\+,\\*,“,”,&,∙,~,\\$,\\^,•,«,»,_,ι,і,‑,‘,’,і]+','',yy_in)\n",
    "#    s_in = re.sub(r'\\W', ' ', y_in)\n",
    "    q_in = re.sub(r'[a-z]*','', yyy_in)                 # удаление латинских букв\n",
    "    \n",
    "    sss_in = re.sub(r'\\b\\w{,2}\\b','', q_in)          # удаление всех слов длиной до 2-х букв\n",
    "    qms_in = re.sub(r'\\s{2,}',' ', q_in)            # замена кратного числа пробелов на один\n",
    "    nms_in = qms_in.strip()\n",
    "#    nms_in = re.sub(r'^[^\\w]*\\s', '', qms_in)         # удаление пробелов в начале строки\n",
    "#    mms_in = re.sub(r'\\s*$','', nms_in)              # удаление пробелов в конце строки\n",
    "    return (nms_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization (f_input_list):          # Лематизация слов в списке\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    lnorm = list()\n",
    "    for word in f_input_list:\n",
    "        p = morph.parse(word)[0]\n",
    "        lnorm.append(p.normal_form)\n",
    "    return (lnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_my_stop_words(word_tokens): # удаление из списка моих стоп-слов\n",
    "    stop_words_nltk = {'который', 'кому', 'имя', 'сегодня', 'вчера', 'завтра', 'также', 'в', 'во', 'свой', \n",
    "                  'это', 'часто', 'зачастую', 'мочь', 'смочь','а', 'без', 'более', 'больше', 'будет', 'будто', \n",
    "                  'бы', 'был', 'была', 'были', 'было', 'быть', 'в', 'вам', 'вас', 'вдруг', 'ведь', 'во', 'вот', \n",
    "                  'впрочем', 'все', 'всегда', 'всего', 'всех', 'всю', 'вы', 'где', 'да', 'даже', 'два', 'для', \n",
    "                  'до', 'другой', 'его', 'ее', 'ей', 'ему', 'если', 'есть', 'еще', 'ж', 'же', 'за', 'зачем', 'здесь', \n",
    "                  'и', 'из', 'или', 'им', 'иногда', 'их', 'к', 'как', 'какая', 'какой', 'когда', 'конечно', 'кто', \n",
    "                  'куда', 'ли', 'лучше', 'между', 'меня', 'мне', 'много', 'может', 'можно', 'мой', 'моя', 'мы', 'на',\n",
    "                  'над', 'надо', 'наконец', 'нас', 'не', 'него', 'нее', 'ней', 'нельзя', 'нет', 'ни', 'нибудь', \n",
    "                  'никогда', 'ним', 'них', 'ничего', 'но', 'ну', 'о', 'об', 'один', 'он', 'она', 'они', 'опять', \n",
    "                  'от', 'перед', 'по', 'под', 'после', 'потом', 'потому', 'почти', 'при', 'про', 'раз', 'разве', \n",
    "                  'с', 'со', 'сам', 'свою', 'себе', 'себя', 'сейчас', 'c', 'со', 'совсем', 'так', 'такой', 'там', 'тебя', \n",
    "                  'тем', 'теперь', 'то', 'тогда', 'того', 'тоже', 'только', 'том', 'тот', 'три', 'тут', 'ты', \n",
    "                  'у', 'уж', 'уже', 'хорошо', 'хоть', 'что', 'чего', 'чем', 'через', 'что', 'чтоб', 'чтобы', 'чуть', \n",
    "                  'эти', 'этого', 'этой', 'этом', 'этот', 'эту', 'я', 'сказал', 'человек', 'жизнь', 'говорил', 'кажется', \n",
    "                  'сказать', 'сегодня', 'сказала', 'сказал'} \n",
    "\n",
    "    my_stop_words = {'сгт','свой', 'стать', 'кроме', 'разный', 'около', 'затем', 'помимо', 'ваш', 'вам', 'некоторый', \n",
    "                     'лишь', 'каждый', 'самый', 'также', 'неоднократно', 'ещё', 'сразу', 'среди',\n",
    "                  'однако', 'вновь', 'иной', 'ныне', 'пока', 'хотя','либо','немного', 'гораздо', 'ничто', 'нередко', 'ныне', \n",
    "                  'наоборот', 'впереди', 'таковой', 'мимо', 'тесно', 'вряд', 'нечто', 'почём', 'почему', 'любой', 'обратно',\n",
    "                  'оттуда', 'очень', 'понапрасну', 'поскольку', 'почему', 'поэтому', 'прежде', 'причём', 'прочий', 'пусть', \n",
    "                  'пока', 'это', 'наш', 'несколько', 'около', 'помимо', 'однако', 'сколько', 'либо', 'гораздо', 'ничто',\n",
    "                   'наоборот', 'никак', 'таковой', 'твой', 'нечто', 'понапрасну', 'почём', 'подробный', 'информация'}\n",
    "\n",
    "    stop_words = stop_words_nltk | my_stop_words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsNERTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "\n",
    "def remove_proper_nouns(text):\n",
    "    # Пропускаем не-строки и пустые строки\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        print(\"text имеет неожидаемый тип\")\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        doc = Doc(text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        \n",
    "        # Если нет сущностей, возвращаем исходный текст\n",
    "        if not doc.spans:\n",
    "            return text\n",
    "        \n",
    "        # Собираем сущности для удаления\n",
    "        spans_to_remove = [span for span in doc.spans if span.type in ['PER', 'LOC', 'ORG']]\n",
    "        \n",
    "        # Если нечего удалять\n",
    "        if not spans_to_remove:\n",
    "            return text\n",
    "            \n",
    "        # Удаляем сущности (начиная с конца)\n",
    "        text_cleaned = text\n",
    "        for span in sorted(spans_to_remove, key=lambda x: x.start, reverse=True):\n",
    "            text_cleaned = text_cleaned[:span.start] + text_cleaned[span.stop:]\n",
    "            \n",
    "        return text_cleaned.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке текста: {text[:50]}... Ошибка: {str(e)}\")\n",
    "        return text  # В случае ошибки возвращаем исходный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_str_of_the_news(string):  # Обработка сырой строки-новости \n",
    "    global abbr_list\n",
    "    string_2 = string_dev(string)     # Предобработка строки\n",
    "    \n",
    "    if len(string_2) != 0:  # Если строка непустая после обработки\n",
    "        list_of_sentences = re.split(r'[.!?]', string_2)\n",
    "        list_of_sentences = [x.split(' ') for x in list_of_sentences if x]\n",
    "        \n",
    "        filtered_sentences = []\n",
    "        for sentence in list_of_sentences:\n",
    "            no_stopwords_1 = del_my_stop_words(sentence)\n",
    "            lemmatized = lematization(no_stopwords_1)\n",
    "            no_stopwords_2 = del_my_stop_words(lemmatized)\n",
    "            text = ' '.join(no_stopwords_2)\n",
    "            # Удаление слов длиной ≤ 2 символов\n",
    "            text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "            # Замена нескольких пробелов на один\n",
    "            text = re.sub(r'\\s{2,}', ' ', text)\n",
    "            text = text.strip()\n",
    "            if text and len(text) > 0:\n",
    "                filtered_sentences.append(text)\n",
    "    \n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read finished\n",
      "Returning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert partition to expected pyarrow schema:\n    `ArrowTypeError(\"Expected bytes, got a 'list' object\", 'Conversion failed for column News_Tokens with type object')`\n\nExpected partition schema:\n    Unnamed: 0: int64\n    News_Id: int64\n    News_Link: large_string\n    News_Date: large_string\n    News_Title: large_string\n    News_Text: large_string\n    News_Tokens: string\n    NumWords: int64\n    NumUnicTokens: int64\n    __null_dask_index__: int64\n\nReceived partition schema:\n    Unnamed: 0: int64\n    News_Id: int64\n    News_Link: large_string\n    News_Date: large_string\n    News_Title: large_string\n    News_Text: large_string\n    News_Tokens: list<item: string>\n      child 0, item: string\n    NumWords: int64\n    NumUnicTokens: int64\n    __null_dask_index__: int64\n\nThis error *may* be resolved by passing in schema information for\nthe mismatched column(s) using the `schema` keyword in `to_parquet`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReturning\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs.to_parquet(\u001b[33m\"\u001b[39m\u001b[33moutput.pq\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mapply_tokenization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mapply_tokenization\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m docs[\u001b[33m'\u001b[39m\u001b[33mNews_Tokens\u001b[39m\u001b[33m'\u001b[39m] = docs[\u001b[33m\"\u001b[39m\u001b[33mNews_Text\u001b[39m\u001b[33m\"\u001b[39m][:\u001b[32m10\u001b[39m].map_partitions(\u001b[38;5;28;01mlambda\u001b[39;00m s: s.apply(process_str_of_the_news), meta=(\u001b[33m\"\u001b[39m\u001b[33mNews_Tokens\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mobject\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReturning\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdocs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput.pq\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:3314\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m   3311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, **kwargs):\n\u001b[32m   3312\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdask_expr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py:661\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m         out = new_collection(\n\u001b[32m    638\u001b[39m             ToParquet(\n\u001b[32m    639\u001b[39m                 df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    657\u001b[39m             )\n\u001b[32m    658\u001b[39m         )\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     out = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[32m    666\u001b[39m fs.invalidate_cache(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\base.py:373\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    350\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    371\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:158\u001b[39m, in \u001b[36mToParquetFunctionWrapper.__call__\u001b[39m\u001b[34m(self, df, block_index)\u001b[39m\n\u001b[32m    151\u001b[39m filename = (\n\u001b[32m    152\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpart.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_i\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.i_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name_function(part_i + \u001b[38;5;28mself\u001b[39m.i_offset)\n\u001b[32m    155\u001b[39m )\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Write out data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_partition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartition_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart_i\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs_pass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:825\u001b[39m, in \u001b[36mArrowDatasetEngine.write_partition\u001b[39m\u001b[34m(cls, df, path, fs, filename, partition_on, return_metadata, fmd, compression, index_cols, schema, head, custom_metadata, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    823\u001b[39m     index_cols = []\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m t = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pandas_to_arrow_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m custom_metadata:\n\u001b[32m    827\u001b[39m     _md = t.schema.metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:786\u001b[39m, in \u001b[36mArrowDatasetEngine._pandas_to_arrow_table\u001b[39m\u001b[34m(cls, df, preserve_index, schema)\u001b[39m\n\u001b[32m    780\u001b[39m expected = textwrap.indent(\n\u001b[32m    781\u001b[39m     schema.to_string(show_schema_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[33m\"\u001b[39m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    782\u001b[39m )\n\u001b[32m    783\u001b[39m actual = textwrap.indent(\n\u001b[32m    784\u001b[39m     df_schema.to_string(show_schema_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[33m\"\u001b[39m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    785\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    787\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to convert partition to expected pyarrow schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    788\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    789\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    790\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected partition schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    791\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    792\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    793\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived partition schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    795\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis error *may* be resolved by passing in schema information for\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    797\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe mismatched column(s) using the `schema` keyword in `to_parquet`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to convert partition to expected pyarrow schema:\n    `ArrowTypeError(\"Expected bytes, got a 'list' object\", 'Conversion failed for column News_Tokens with type object')`\n\nExpected partition schema:\n    Unnamed: 0: int64\n    News_Id: int64\n    News_Link: large_string\n    News_Date: large_string\n    News_Title: large_string\n    News_Text: large_string\n    News_Tokens: string\n    NumWords: int64\n    NumUnicTokens: int64\n    __null_dask_index__: int64\n\nReceived partition schema:\n    Unnamed: 0: int64\n    News_Id: int64\n    News_Link: large_string\n    News_Date: large_string\n    News_Title: large_string\n    News_Text: large_string\n    News_Tokens: list<item: string>\n      child 0, item: string\n    NumWords: int64\n    NumUnicTokens: int64\n    __null_dask_index__: int64\n\nThis error *may* be resolved by passing in schema information for\nthe mismatched column(s) using the `schema` keyword in `to_parquet`."
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "def apply_tokenization():\n",
    "    docs = dd.read_parquet(\"raw-data.pq\")\n",
    "    print(\"Read finished\")\n",
    "    docs['News_Tokens'] = docs[\"News_Text\"][:10].map_partitions(lambda s: s.apply(process_str_of_the_news), meta=(\"News_Tokens\", object))\n",
    "    print(\"Returning\")\n",
    "    schema = {\n",
    "        \"News_Text\": pa.string(),\n",
    "        \"News_Tokens\": pa.list_(pa.string()),\n",
    "        \"News_Title\": pa.string()\n",
    "    }\n",
    "    return docs.to_parquet(\"output.pq\")\n",
    "apply_tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.to_csv(\"data_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['News_Tokens'] = docs['News_Tokens'].apply(\n",
    "    lambda sentence_list: [remove_proper_nouns(sent) for sent in sentence_list]\n",
    ")\n",
    "docs.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_excel(\"News_SGU_31077_Processed_1.xlsx\")\n",
    "docs.to_parquet(\"raw-data.pq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
